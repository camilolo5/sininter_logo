{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b7f4476-f406-48f3-9805-8d7e0c30b8c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# SinIntermediarios â€” Build Silver (Dashboard)\n",
    "\n",
    "**Purpose**\n",
    "- Read the product catalog (`MAIN_TABLE`).\n",
    "- Read the daily scrape snapshot (`SCRAPE_TABLE`) for a chosen `TARGET_SCRAPE_DATE` (defaults to latest available).\n",
    "- Join + standardize schema.\n",
    "- Write:\n",
    "  - `SILVER_HISTORY_TABLE` (MERGE/upsert by (scrape_date, url))\n",
    "  - `SILVER_DASHBOARD_TABLE` (overwrite snapshot for dashboards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63b75ab3-67c7-4489-8241-b8a475014c79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install beautifulsoup4 lxml\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5bfc207-dba2-423f-993d-f06649e24341",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1) CONFIG\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "MAIN_TABLE  = \"workspace.sinintermediarios.main_file\"\n",
    "SCRAPE_TABLE = \"workspace.sinintermediarios.bronze_scrape_daily\"\n",
    "\n",
    "SILVER_HISTORY_TABLE = \"workspace.sinintermediarios.silver_all_history\"\n",
    "SILVER_DASHBOARD_TABLE = \"workspace.sinintermediarios.silver_dashboard_latest\"\n",
    "\n",
    "TARGET_SCRAPE_DATE = None  # 'YYYY-MM-DD' to force, else latest in SCRAPE_TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1f68c33-25ce-410d-a2cb-5d902a0c6ee7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Helpers: HTTP, parsing, normalization\n",
    "# --------------------------------------------------------------------------------------\n",
    "import datetime\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, Optional\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "DEFAULT_HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "}\n",
    "\n",
    "\n",
    "def soup(html: str) -> BeautifulSoup:\n",
    "    return BeautifulSoup(html or \"\", \"lxml\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FetchResult:\n",
    "    status: str\n",
    "    error: str\n",
    "    http_status: int\n",
    "    elapsed_ms: int\n",
    "    html: str\n",
    "\n",
    "    @property\n",
    "    def status_code(self) -> int:\n",
    "        return self.http_status\n",
    "\n",
    "    @property\n",
    "    def text(self) -> str:\n",
    "        return self.html\n",
    "\n",
    "\n",
    "class HttpClient:\n",
    "    \"\"\"Tiny wrapper to keep scraper code consistent and add minimal resiliency.\"\"\"\n",
    "\n",
    "    def __init__(self, headers: Optional[dict] = None):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update(headers or DEFAULT_HEADERS)\n",
    "\n",
    "    def get(self, url: str, timeout_s: int = 25) -> FetchResult:\n",
    "        t0 = time.time()\n",
    "        try:\n",
    "            r = self.session.get(url, timeout=timeout_s, allow_redirects=True)\n",
    "            return FetchResult(\n",
    "                status=\"ok\",\n",
    "                error=\"\",\n",
    "                http_status=int(r.status_code),\n",
    "                elapsed_ms=int((time.time() - t0) * 1000),\n",
    "                html=r.text or \"\",\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return FetchResult(\n",
    "                status=\"exception\",\n",
    "                error=f\"{type(e).__name__}: {e}\",\n",
    "                http_status=0,\n",
    "                elapsed_ms=int((time.time() - t0) * 1000),\n",
    "                html=\"\",\n",
    "            )\n",
    "\n",
    "\n",
    "HTTP = HttpClient()\n",
    "\n",
    "\n",
    "def default_result(site: Optional[str] = None, url: Optional[str] = None) -> Dict[str, Any]:\n",
    "    \"\"\"Canonical result shape for ALL scrapers.\"\"\"\n",
    "    return {\n",
    "        \"site\": site,\n",
    "        \"url\": url,\n",
    "        \"status\": \"ok\",\n",
    "        \"error\": None,\n",
    "        \"http_status\": None,\n",
    "        \"elapsed_ms\": None,\n",
    "        \"scraped_at\": None,\n",
    "        # raw prices (floats) - normalized later\n",
    "        \"price\": 0.0,          # regular / single purchase\n",
    "        \"promo_price\": 0.0,    # discounted\n",
    "        \"membership\": 0.0,     # subscription / membership\n",
    "        \"has_stock\": None,\n",
    "        \"stock\": None,\n",
    "        \"name_scraped\": None,\n",
    "        \"image_url\": None,\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_og_image_url(sp: BeautifulSoup) -> str:\n",
    "    for sel in [\n",
    "        'meta[property=\"og:image\"]',\n",
    "        'meta[name=\"og:image\"]',\n",
    "        'meta[name=\"twitter:image\"]',\n",
    "        'meta[property=\"twitter:image\"]',\n",
    "    ]:\n",
    "        tag = sp.select_one(sel)\n",
    "        if tag and tag.get(\"content\"):\n",
    "            u = tag[\"content\"].strip()\n",
    "            if u.startswith(\"//\"):\n",
    "                u = \"https:\" + u\n",
    "            return u\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def normalize_site(site_raw: Optional[str]) -> str:\n",
    "    if site_raw is None:\n",
    "        return \"\"\n",
    "    s = str(site_raw).strip().lower()\n",
    "    s = s.replace(\"_\", \" \").replace(\"-\", \" \")\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "    mapping = {\n",
    "        \"savvy\": \"savvy\",\n",
    "        \"mercado libre\": \"mercadolibre\",\n",
    "        \"proscience\": \"proscience\",\n",
    "        \"zona fit\": \"zonafit\",\n",
    "        \"sin intermediarios\": \"sinintermediarios\",\n",
    "        \"vitanas\": \"vitanas\",\n",
    "        \"farmatodo\": \"farmatodo\",\n",
    "        \"herbivore\": \"herbivore\",\n",
    "        \"nutramerican\": \"nutramerican\",\n",
    "        \"colsubsidio\": \"colsubsidio\",\n",
    "        \"muscletech\": \"muscletech\",\n",
    "    }\n",
    "    return mapping.get(s, s.replace(\" \", \"\"))\n",
    "\n",
    "\n",
    "def clean_url(url: Optional[str]) -> str:\n",
    "    if url is None:\n",
    "        return \"\"\n",
    "    u = str(url).strip()\n",
    "    if u.lower() in (\"nan\", \"none\", \"\"):\n",
    "        return \"\"\n",
    "    return u.split(\"#\")[0].strip()\n",
    "\n",
    "\n",
    "def parse_price_any(val: Any) -> float:\n",
    "    \"\"\"Parse common price strings into a float.\n",
    "\n",
    "    Handles:\n",
    "      - numbers (int/float)\n",
    "      - COP strings with thousand separators: \"$174.100\" / \"$195,900\" / \"174.100,00\"\n",
    "      - Shopify JSON strings: \"164990.00\"\n",
    "    \"\"\"\n",
    "    if val is None:\n",
    "        return 0.0\n",
    "    if isinstance(val, (int, float)):\n",
    "        try:\n",
    "            return float(val)\n",
    "        except Exception:\n",
    "            return 0.0\n",
    "\n",
    "    s = str(val).strip()\n",
    "    if s == \"\":\n",
    "        return 0.0\n",
    "\n",
    "    # Keep only digits + separators\n",
    "    s2 = re.sub(r\"[^0-9\\,\\.]\", \"\", s)\n",
    "    if s2 == \"\":\n",
    "        return 0.0\n",
    "\n",
    "    # If it looks like COP with thousand separators and optional cents, normalize to pesos\n",
    "    # e.g., \"174.100\" or \"174.100,00\" or \"195,900\"\n",
    "    has_cents = bool(re.search(r\"[\\,\\.]\\d{2}$\", s2))\n",
    "\n",
    "    # Determine decimal separator if both present\n",
    "    if \",\" in s2 and \".\" in s2:\n",
    "        # decimal separator is the last occurring\n",
    "        if s2.rfind(\",\") > s2.rfind(\".\"):\n",
    "            # \".\" thousand, \",\" decimal\n",
    "            s2 = s2.replace(\".\", \"\").replace(\",\", \".\")\n",
    "        else:\n",
    "            # \",\" thousand, \".\" decimal\n",
    "            s2 = s2.replace(\",\", \"\")\n",
    "    elif \",\" in s2 and \".\" not in s2:\n",
    "        # Could be thousand or decimal. If ends with 2 decimals -> decimal; else thousand.\n",
    "        if has_cents:\n",
    "            s2 = s2.replace(\",\", \".\")\n",
    "        else:\n",
    "            s2 = s2.replace(\",\", \"\")\n",
    "    # else: only '.' or none -> float will handle\n",
    "\n",
    "    try:\n",
    "        x = float(s2)\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "    # If cents were explicitly present, convert to pesos by flooring cents.\n",
    "    # (For COP, cents are not used in practice; Shopify may emit \".00\".)\n",
    "    if has_cents:\n",
    "        x = float(int(x))\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def cop_thousands_fix(x: float) -> float:\n",
    "    \"\"\"Heuristic: COP prices rarely in 1..999 range; if they are, it's often missing thousands.\"\"\"\n",
    "    try:\n",
    "        v = float(x or 0.0)\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "    if 1 < v < 1000:\n",
    "        return v * 1000\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b8dd59f-6826-478f-b3e5-e1c9eade5e72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2) LOAD MAIN + SCRAPE (target date)\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "main_df = spark.table(MAIN_TABLE)\n",
    "scrape_df_all = spark.table(SCRAPE_TABLE)\n",
    "\n",
    "# pick latest scrape_date if not provided\n",
    "if TARGET_SCRAPE_DATE:\n",
    "    target_date = TARGET_SCRAPE_DATE\n",
    "else:\n",
    "    target_date = (scrape_df_all.select(F.max('scrape_date').alias('d')).collect()[0]['d'])\n",
    "\n",
    "print('Using scrape_date:', target_date)\n",
    "\n",
    "scrape_df = scrape_df_all.filter(F.col('scrape_date') == F.lit(target_date))\n",
    "\n",
    "# De-dup: keep latest scraped_at per url (prefer status ok)\n",
    "# status priority: ok > no_price > others\n",
    "status_rank = F.when(F.col('status')=='ok', F.lit(3))\n",
    "status_rank = status_rank.when(F.col('status')=='no_price', F.lit(2))\n",
    "status_rank = status_rank.otherwise(F.lit(1))\n",
    "\n",
    "w = Window.partitionBy('url').orderBy(status_rank.desc(), F.to_timestamp('scraped_at').desc_nulls_last())\n",
    "scrape_df = (scrape_df\n",
    "    .withColumn('_rn', F.row_number().over(w))\n",
    "    .filter(F.col('_rn')==1)\n",
    "    .drop('_rn')\n",
    ")\n",
    "\n",
    "print('Main rows:', main_df.count())\n",
    "print('Scrape rows (dedup):', scrape_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90e8d2e7-4acb-4151-9090-b2f33b9fa63f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3) JOIN + CLEAN  (FIX: avoid ambiguous 'site' column)\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# --- resolve main columns robustly ---\n",
    "cols_lc = {c.lower(): c for c in main_df.columns}\n",
    "\n",
    "url_col = cols_lc.get(\"url\")\n",
    "site_raw_col = cols_lc.get(\"comercio\") or cols_lc.get(\"site\")  # you said site should be comercio\n",
    "\n",
    "if not url_col:\n",
    "    raise ValueError(f\"MAIN_TABLE must contain 'url'. Found: {main_df.columns}\")\n",
    "\n",
    "if not site_raw_col:\n",
    "    main_df = main_df.withColumn(\"_site_raw_fallback\", F.lit(None))\n",
    "    site_raw_col = \"_site_raw_fallback\"\n",
    "\n",
    "# --- avoid name collision: rename main 'site' if it exists ---\n",
    "# If main_df already has a column literally called \"site\", rename it so we don't collide.\n",
    "if \"site\" in main_df.columns:\n",
    "    main_df = main_df.withColumnRenamed(\"site\", \"site_main\")\n",
    "\n",
    "m = main_df.alias(\"m\")\n",
    "s = scrape_df.alias(\"s\")\n",
    "\n",
    "# --- join on trimmed URL ---\n",
    "joined = (\n",
    "    m.join(\n",
    "        s,\n",
    "        on=(F.trim(F.col(f\"m.{url_col}\")) == F.trim(F.col(\"s.url\"))),\n",
    "        how=\"left\",\n",
    "    )\n",
    "    .withColumn(\"site_raw\", F.trim(F.col(f\"m.{site_raw_col}\")))\n",
    ")\n",
    "\n",
    "# --- normalized site: prefer scrape site, else normalize from site_raw ---\n",
    "normalize_site_udf = F.udf(normalize_site, StringType())\n",
    "\n",
    "joined = joined.withColumn(\n",
    "    \"site_norm\",\n",
    "    F.coalesce(F.col(\"s.site\"), normalize_site_udf(F.col(\"site_raw\")))\n",
    ")\n",
    "\n",
    "# --- keep all original product metadata (including comercio, marca, gramos_empaque, etc.) ---\n",
    "# IMPORTANT: do not drop url_col or site_raw_col from meta; we keep them as-is in metadata\n",
    "meta_cols = [c for c in main_df.columns if c != url_col]  # url is normalized separately\n",
    "\n",
    "clean_df = joined.select(\n",
    "    F.col(\"site_raw\"),\n",
    "    F.col(\"site_norm\").alias(\"site\"),\n",
    "    F.trim(F.col(f\"m.{url_col}\")).alias(\"url\"),\n",
    "\n",
    "    F.col(\"s.status\").alias(\"status\"),\n",
    "    F.col(\"s.error\").alias(\"error\"),\n",
    "    F.col(\"s.http_status\").alias(\"http_status\"),\n",
    "    F.col(\"s.elapsed_ms\").alias(\"elapsed_ms\"),\n",
    "    F.col(\"s.scraped_at\").alias(\"scraped_at\"),\n",
    "    F.col(\"s.scrape_date\").alias(\"scrape_date\"),\n",
    "\n",
    "    F.col(\"s.price_full_cop\").alias(\"price_full_cop\"),\n",
    "    F.col(\"s.price_discount_cop\").alias(\"price_discount_cop\"),\n",
    "    F.col(\"s.price_membership_cop\").alias(\"price_membership_cop\"),\n",
    "    F.col(\"s.price_cop\").alias(\"price_cop\"),\n",
    "\n",
    "    F.col(\"s.has_stock\").alias(\"has_stock\"),\n",
    "    F.col(\"s.stock\").alias(\"stock\"),\n",
    "    F.col(\"s.image_url\").alias(\"image_url\"),\n",
    "    F.col(\"s.name_scraped\").alias(\"name_scraped\"),\n",
    "\n",
    "    *[F.col(f\"m.{c}\").alias(c) for c in meta_cols if c not in [\"site_raw\", \"site_norm\"]]\n",
    ")\n",
    "\n",
    "display(clean_df.limit(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e24b27e-4a9b-4537-b2c2-34d6066649f4",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"url\":{\"format\":{\"preset\":\"string-preset-url\",\"locale\":\"en\"}},\"image_url\":{\"format\":{\"preset\":\"string-preset-url\",\"locale\":\"en\"}}}},\"syncTimestamp\":1769117433538}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4) WRITE SILVER TABLES (snapshot + history create/merge)\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def table_exists(full_name: str) -> bool:\n",
    "    # Most reliable across DBR / Spark Connect / UC\n",
    "    try:\n",
    "        spark.sql(f\"DESCRIBE TABLE {full_name}\")\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "print(\"SILVER_DASHBOARD_TABLE =\", SILVER_DASHBOARD_TABLE)\n",
    "print(\"SILVER_HISTORY_TABLE   =\", SILVER_HISTORY_TABLE)\n",
    "\n",
    "# -------------------------\n",
    "# 0) Defensive de-dupe: 1 row per (scrape_date, url)\n",
    "# -------------------------\n",
    "order_col = F.col(\"scraped_at\").desc() if \"scraped_at\" in clean_df.columns else F.current_timestamp().desc()\n",
    "w = Window.partitionBy(\"scrape_date\", \"url\").orderBy(order_col)\n",
    "\n",
    "clean_hist = (\n",
    "    clean_df\n",
    "      .withColumn(\"_rn\", F.row_number().over(w))\n",
    "      .filter(F.col(\"_rn\") == 1)\n",
    "      .drop(\"_rn\")\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 4a) Snapshot (overwrite)\n",
    "# -------------------------\n",
    "(\n",
    "    clean_df\n",
    "      .write.format(\"delta\")\n",
    "      .mode(\"overwrite\")\n",
    "      .option(\"overwriteSchema\", \"true\")\n",
    "      .saveAsTable(SILVER_DASHBOARD_TABLE)\n",
    ")\n",
    "print(\"Wrote dashboard snapshot:\", SILVER_DASHBOARD_TABLE)\n",
    "\n",
    "# -------------------------\n",
    "# 4b) History (create if missing, else schema-safe merge)\n",
    "# -------------------------\n",
    "if not table_exists(SILVER_HISTORY_TABLE):\n",
    "    (\n",
    "        clean_hist\n",
    "          .write.format(\"delta\")\n",
    "          .mode(\"overwrite\")\n",
    "          .partitionBy(\"scrape_date\")\n",
    "          .saveAsTable(SILVER_HISTORY_TABLE)\n",
    "    )\n",
    "    print(\"Created history table:\", SILVER_HISTORY_TABLE)\n",
    "\n",
    "# else:\n",
    "#     tgt = spark.table(SILVER_HISTORY_TABLE)\n",
    "#     tgt_cols = tgt.columns\n",
    "\n",
    "#     # Align src to target schema (add missing cols as nulls, drop extras)\n",
    "#     src_aligned = clean_hist\n",
    "#     for c in tgt_cols:\n",
    "#         if c not in src_aligned.columns:\n",
    "#             src_aligned = src_aligned.withColumn(c, F.lit(None).cast(tgt.schema[c].dataType))\n",
    "#     src_aligned = src_aligned.select(*tgt_cols)\n",
    "\n",
    "#     src_aligned.createOrReplaceTempView(\"src_aligned\")\n",
    "\n",
    "#     update_set = \",\\n      \".join([f\"t.`{c}` = s.`{c}`\" for c in tgt_cols if c not in [\"scrape_date\", \"url\"]])\n",
    "#     insert_cols = \", \".join([f\"`{c}`\" for c in tgt_cols])\n",
    "#     insert_vals = \", \".join([f\"s.`{c}`\" for c in tgt_cols])\n",
    "\n",
    "#     spark.sql(f\"\"\"\n",
    "#       MERGE INTO {SILVER_HISTORY_TABLE} AS t\n",
    "#       USING src_aligned AS s\n",
    "#       ON t.scrape_date = s.scrape_date AND t.url = s.url\n",
    "#       WHEN MATCHED THEN UPDATE SET\n",
    "#         {update_set}\n",
    "#       WHEN NOT MATCHED THEN INSERT ({insert_cols})\n",
    "#       VALUES ({insert_vals})\n",
    "#     \"\"\")\n",
    "\n",
    "else:\n",
    "    tgt = spark.table(SILVER_HISTORY_TABLE)\n",
    "    tgt_cols = tgt.columns\n",
    "\n",
    "    # 1) EVOLVE TARGET SCHEMA: add any new columns from source into target table\n",
    "    src_schema = {f.name: f.dataType.simpleString() for f in clean_hist.schema.fields}\n",
    "    new_cols = [c for c in src_schema.keys() if c not in tgt_cols]\n",
    "\n",
    "    if new_cols:\n",
    "        add_cols_ddl = \", \".join([f\"`{c}` {src_schema[c]}\" for c in new_cols])\n",
    "        spark.sql(f\"ALTER TABLE {SILVER_HISTORY_TABLE} ADD COLUMNS ({add_cols_ddl})\")\n",
    "        tgt = spark.table(SILVER_HISTORY_TABLE)\n",
    "        tgt_cols = tgt.columns\n",
    "\n",
    "    # 2) Align source to updated target schema (add missing cols as null, order cols)\n",
    "    src_aligned = clean_hist\n",
    "    tgt_schema = tgt.schema\n",
    "\n",
    "    for c in tgt_cols:\n",
    "        if c not in src_aligned.columns:\n",
    "            src_aligned = src_aligned.withColumn(c, F.lit(None).cast(tgt_schema[c].dataType))\n",
    "\n",
    "    src_aligned = src_aligned.select(*tgt_cols)\n",
    "    src_aligned.createOrReplaceTempView(\"src_aligned\")\n",
    "\n",
    "    update_set = \",\\n      \".join([f\"t.`{c}` = s.`{c}`\" for c in tgt_cols if c not in [\"scrape_date\", \"url\"]])\n",
    "    insert_cols = \", \".join([f\"`{c}`\" for c in tgt_cols])\n",
    "    insert_vals = \", \".join([f\"s.`{c}`\" for c in tgt_cols])\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "      MERGE INTO {SILVER_HISTORY_TABLE} AS t\n",
    "      USING src_aligned AS s\n",
    "      ON t.scrape_date = s.scrape_date AND t.url = s.url\n",
    "      WHEN MATCHED THEN UPDATE SET\n",
    "        {update_set}\n",
    "      WHEN NOT MATCHED THEN INSERT ({insert_cols})\n",
    "      VALUES ({insert_vals})\n",
    "    \"\"\")\n",
    "    print(\"Upserted history table:\", SILVER_HISTORY_TABLE)\n",
    "\n",
    "# -------------------------\n",
    "# Peek\n",
    "# -------------------------\n",
    "display(\n",
    "    spark.table(SILVER_HISTORY_TABLE)\n",
    "      .orderBy(F.col(\"scrape_date\").desc(), F.col(\"site\").asc_nulls_last())\n",
    "      .limit(200)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_SinIntermediarios_Build_Silver_Dashboard",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
