{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9acd4bf-007e-498b-b6f5-d70dbae76d63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# SinIntermediarios — Daily Scrape (Bronze)\n",
    "\n",
    "**Purpose**\n",
    "- Read the URL catalog from `MAIN_TABLE`.\n",
    "- Scrape prices (regular / promo / membership when available) + status, stock, image.\n",
    "- Write an **append-only daily snapshot** to `SCRAPE_TABLE` partitioned by `scrape_date`.\n",
    "\n",
    "**Outputs**\n",
    "- `SCRAPE_TABLE` contains one row per (scrape_date, url) with the latest scrape result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a94781c3-b71e-4562-a031-36726725b651",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 0) (Optional) Dependencies\n",
    "# In Databricks notebooks, prefer %pip so packages persist on the cluster.\n",
    "# Uncomment if needed.\n",
    "# %pip install -q beautifulsoup4 lxml requests pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7bc030a-44b9-4eb5-b7e6-0ed6d16d7cc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install beautifulsoup4 lxml\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fe6d7c2-0f14-4088-b54b-d973a674dc54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1) CONFIG\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "MAIN_TABLE  = \"workspace.sinintermediarios.main_file\"\n",
    "SCRAPE_TABLE = \"workspace.sinintermediarios.bronze_scrape_daily\"\n",
    "\n",
    "INPUT_LIMIT = None  # int for testing\n",
    "REQUEST_SLEEP_SECONDS = 0.25\n",
    "\n",
    "# For safety: parallelize by SITE, not by URL (avoids hammering a single domain).\n",
    "MAX_WORKERS = 6\n",
    "\n",
    "# If you re-run in the same day, the notebook will de-dup per (scrape_date, url) before writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5289572b-2b51-4e8e-95f5-11216794e824",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2) LOAD INPUT URLS\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "main_df = spark.table(MAIN_TABLE)\n",
    "\n",
    "cols_lc = {c.lower(): c for c in main_df.columns}\n",
    "url_col = cols_lc.get('url')\n",
    "site_col = cols_lc.get('comercio')\n",
    "\n",
    "if not url_col:\n",
    "    raise ValueError(f\"MAIN_TABLE must contain 'url'. Found: {main_df.columns}\")\n",
    "if not site_col:\n",
    "    raise ValueError(f\"MAIN_TABLE must contain 'site' or 'comercio'. Found: {main_df.columns}\")\n",
    "\n",
    "input_df = (\n",
    "    main_df\n",
    "      .select(\n",
    "          F.trim(F.col(site_col)).alias('site_raw'),\n",
    "          F.trim(F.col(url_col)).alias('url_raw')\n",
    "      )\n",
    "      .filter(F.col('url_raw').isNotNull() & (F.length('url_raw') > 0))\n",
    "      .dropDuplicates(['url_raw'])\n",
    ")\n",
    "\n",
    "if INPUT_LIMIT:\n",
    "    input_df = input_df.limit(int(INPUT_LIMIT))\n",
    "\n",
    "display(input_df.limit(50))\n",
    "print('Rows to scrape:', input_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0ab2571-5f18-4557-9a0a-89ed43587b5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "# Databricks notebook source\n# --------------------------------------------------------------------------------------\n# Helpers: HTTP, parsing, normalization\n# --------------------------------------------------------------------------------------\nimport datetime\nimport json\nimport re\nimport time\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, Optional\nfrom urllib.parse import urlparse, parse_qs\n\nimport requests\nfrom bs4 import BeautifulSoup\n\nDEFAULT_HEADERS = {\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n    \"Accept-Language\": \"es-CO,es;q=0.9\",\n    \"Referer\": \"https://www.google.com/\",\n    \"Connection\": \"keep-alive\",\n}\n\n\ndef soup(html: str) -> BeautifulSoup:\n    return BeautifulSoup(html or \"\", \"lxml\")\n\n\n@dataclass\nclass FetchResult:\n    status: str\n    error: str\n    http_status: int\n    elapsed_ms: int\n    html: str\n\n    @property\n    def status_code(self) -> int:\n        return self.http_status\n\n    @property\n    def text(self) -> str:\n        return self.html\n\n\nclass HttpClient:\n    \"\"\"Tiny wrapper to keep scraper code consistent and add minimal resiliency.\"\"\"\n\n    def __init__(self, headers: Optional[dict] = None):\n        self.session = requests.Session()\n        self.session.headers.update(headers or DEFAULT_HEADERS)\n\n    def get(self, url: str, timeout_s: int = 25, retries: int = 2, backoff_s: int = 3) -> FetchResult:\n        t0 = time.time()\n        last_error = \"\"\n        last_status = 0\n        for attempt in range(retries + 1):\n            try:\n                r = self.session.get(url, timeout=timeout_s, allow_redirects=True)\n                if r.status_code == 200:\n                    return FetchResult(\n                        status=\"ok\",\n                        error=\"\",\n                        http_status=int(r.status_code),\n                        elapsed_ms=int((time.time() - t0) * 1000),\n                        html=r.text or \"\",\n                    )\n                last_status = int(r.status_code)\n                last_error = f\"HTTP {r.status_code}\"\n            except Exception as e:\n                last_error = f\"{type(e).__name__}: {e}\"\n                last_status = 0\n            if attempt < retries:\n                time.sleep(backoff_s * (attempt + 1))\n        return FetchResult(\n            status=\"exception\" if last_status == 0 else \"http_error\",\n            error=last_error,\n            http_status=last_status,\n            elapsed_ms=int((time.time() - t0) * 1000),\n            html=\"\",\n        )\n\n\nHTTP = HttpClient()\n\n\ndef default_result(site: Optional[str] = None, url: Optional[str] = None) -> Dict[str, Any]:\n    \"\"\"Canonical result shape for ALL scrapers.\"\"\"\n    return {\n        \"site\": site,\n        \"url\": url,\n        \"status\": \"ok\",\n        \"error\": None,\n        \"http_status\": None,\n        \"elapsed_ms\": None,\n        \"scraped_at\": None,\n        # raw prices (floats) - normalized later\n        \"price\": 0.0,          # regular / single purchase\n        \"promo_price\": 0.0,    # discounted\n        \"membership\": 0.0,     # subscription / membership\n        \"has_stock\": None,\n        \"stock\": None,\n        \"name_scraped\": None,\n        \"image_url\": None,\n    }\n\n# ---------- Site normalization ----------\ndef normalize_site(site_raw: str) -> str:\n    if not site_raw:\n        return None\n    s = site_raw.strip().lower()\n\n    if \"mercado\" in s:\n        return \"mercadolibre\"\n    if \"sin intermediarios\" in s:\n        return \"sinintermediarios\"\n    if \"farmatodo\" in s:\n        return \"farmatodo\"\n    if \"vitanas\" in s:\n        return \"vitanas\"\n    if \"nutramerican\" in s:\n        return \"nutramerican\"\n    if \"zonafit\" in s or \"zona fit\" in s:\n        return \"zonafit\"\n    if \"proscience\" in s:\n        return \"proscience\"\n    if \"herbivore\" in s:\n        return \"herbivore\"\n    if \"colsubsidio\" in s:\n        return \"colsubsidio\"\n    if \"savvy\" in s:\n        return \"savvy\"\n\n    return s.replace(\" \", \"\")\n\ndef extract_og_image_url(sp: BeautifulSoup) -> str:\n    for sel in [\n        'meta[property=\"og:image\"]',\n        'meta[name=\"og:image\"]',\n        'meta[name=\"twitter:image\"]',\n        'meta[property=\"twitter:image\"]',\n    ]:\n        tag = sp.select_one(sel)\n        if tag and tag.get(\"content\"):\n            u = tag[\"content\"].strip()\n            if u.startswith(\"//\"):\n                u = \"https:\" + u\n            return u\n    return \"\"\n\n\ndef normalize_site(site_raw: Optional[str]) -> str:\n    if site_raw is None:\n        return \"\"\n    s = str(site_raw).strip().lower()\n    s = s.replace(\"_\", \" \").replace(\"-\", \" \")\n    s = re.sub(r\"\\s+\", \" \", s).strip()\n\n    mapping = {\n        \"savvy\": \"savvy\",\n        \"mercado libre\": \"mercadolibre\",\n        \"proscience\": \"proscience\",\n        \"zona fit\": \"zonafit\",\n        \"sin intermediarios\": \"sinintermediarios\",\n        \"vitanas\": \"vitanas\",\n        \"farmatodo\": \"farmatodo\",\n        \"herbivore\": \"herbivore\",\n        \"nutramerican\": \"nutramerican\",\n        \"colsubsidio\": \"colsubsidio\",\n        \"muscletech\": \"muscletech\",\n    }\n    return mapping.get(s, s.replace(\" \", \"\"))\n\n\ndef clean_url(url: Optional[str]) -> str:\n    if url is None:\n        return \"\"\n    u = str(url).strip()\n    if u.lower() in (\"nan\", \"none\", \"\"):\n        return \"\"\n    return u.split(\"#\")[0].strip()\n\n\ndef parse_price_any(val: Any) -> float:\n    \"\"\"Parse common price strings into a float.\n\n    Handles:\n      - numbers (int/float)\n      - COP strings with thousand separators: \"$174.100\" / \"$195,900\" / \"174.100,00\"\n      - Shopify JSON strings: \"164990.00\"\n    \"\"\"\n    if val is None:\n        return 0.0\n    if isinstance(val, (int, float)):\n        try:\n            return float(val)\n        except Exception:\n            return 0.0\n\n    s = str(val).strip()\n    if s == \"\":\n        return 0.0\n\n    # Keep only digits + separators\n    s2 = re.sub(r\"[^0-9\\,\\.]\", \"\", s)\n    if s2 == \"\":\n        return 0.0\n\n    # If it looks like COP with thousand separators and optional cents, normalize to pesos\n    # e.g., \"174.100\" or \"174.100,00\" or \"195,900\"\n    has_cents = bool(re.search(r\"[\\,\\.]\\d{2}$\", s2))\n\n    # Determine decimal separator if both present\n    if \",\" in s2 and \".\" in s2:\n        # decimal separator is the last occurring\n        if s2.rfind(\",\") > s2.rfind(\".\"):\n            # \".\" thousand, \",\" decimal\n            s2 = s2.replace(\".\", \"\").replace(\",\", \".\")\n        else:\n            # \",\" thousand, \".\" decimal\n            s2 = s2.replace(\",\", \"\")\n    elif \",\" in s2 and \".\" not in s2:\n        # Could be thousand or decimal. If ends with 2 decimals -> decimal; else thousand.\n        if has_cents:\n            s2 = s2.replace(\",\", \".\")\n        else:\n            s2 = s2.replace(\",\", \"\")\n    # else: only '.' or none -> float will handle\n\n    try:\n        x = float(s2)\n    except Exception:\n        return 0.0\n\n    # If cents were explicitly present, convert to pesos by flooring cents.\n    # (For COP, cents are not used in practice; Shopify may emit \".00\".)\n    if has_cents:\n        x = float(int(x))\n\n    return x\n\ndef parse_price_cop(val):\n    # Alias consistente para COP: devuelve pesos ENTEROS (sin decimales)\n    return float(int(parse_price_any(val)))\n\n\ndef cop_thousands_fix(x: float) -> float:\n    \"\"\"Heuristic: COP prices rarely in 1..999 range; if they are, it's often missing thousands.\"\"\"\n    try:\n        v = float(x or 0.0)\n    except Exception:\n        return 0.0\n    if 1 < v < 1000:\n        return v * 1000\n    return v\n\ndef http_get(url: str, headers: dict | None = None):\n    \"\"\"\n    Your HttpClient wrapper uses: HTTP.get(url, extra_headers=...)\n    \"\"\"\n    return HTTP.get(url, extra_headers=(headers or {}))\n"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8454d68-69f5-4f65-9bc0-760e5f8bedfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------\n# Scrapers\n# --------------------------------------------------------------------------------------\n\n# --- Generic Shopify / WooCommerce fallbacks (used for some sites) ---\n\ndef scrape_shopify_basic(url: str, site_key: str) -> dict:\n    out = default_result(site_key, url)\n    fr = HTTP.get(url)\n    out[\"http_status\"], out[\"elapsed_ms\"] = fr.status_code, fr.elapsed_ms\n\n    if fr.error:\n        out[\"status\"], out[\"error\"] = \"exception\", fr.error\n        return out\n    if fr.status_code >= 400:\n        out[\"status\"], out[\"error\"] = \"http_error\", f\"http_status={fr.status_code}\"\n        return out\n\n    sp = soup(fr.text)\n    out[\"image_url\"] = extract_og_image_url(sp)\n\n    # JSON-LD offers.price\n    price = 0.0\n    for tag in sp.find_all(\"script\", attrs={\"type\": \"application/ld+json\"}):\n        try:\n            data = json.loads(tag.get_text(strip=True) or \"{}\")\n            candidates = data if isinstance(data, list) else [data]\n            for d in candidates:\n                offers = d.get(\"offers\")\n                if isinstance(offers, dict) and offers.get(\"price\") is not None:\n                    price = parse_price_any(offers.get(\"price\"))\n                    break\n                if isinstance(offers, list):\n                    for o in offers:\n                        if isinstance(o, dict) and o.get(\"price\") is not None:\n                            price = parse_price_any(o.get(\"price\"))\n                            break\n                if price:\n                    break\n        except Exception:\n            pass\n        if price:\n            break\n\n    if not price:\n        meta = sp.find(\"meta\", attrs={\"property\": \"product:price:amount\"})\n        if meta and meta.get(\"content\"):\n            price = parse_price_any(meta.get(\"content\"))\n\n    out[\"price\"] = float(price or 0.0)\n    out[\"status\"] = \"ok\" if out[\"price\"] > 0 else \"no_price\"\n    return out\n\n\ndef scrape_woocommerce_basic(url: str, site_key: str) -> dict:\n    out = default_result(site_key, url)\n    fr = HTTP.get(url)\n    out[\"http_status\"], out[\"elapsed_ms\"] = fr.status_code, fr.elapsed_ms\n\n    if fr.error:\n        out[\"status\"], out[\"error\"] = \"exception\", fr.error\n        return out\n    if fr.status_code >= 400:\n        out[\"status\"], out[\"error\"] = \"http_error\", f\"http_status={fr.status_code}\"\n        return out\n\n    sp = soup(fr.text)\n\n    price = 0.0\n    price_block = sp.select_one(\"p.price bdi\") or sp.select_one(\"span.woocommerce-Price-amount bdi\")\n    if price_block:\n        price = parse_price_any(price_block.get_text(\" \", strip=True))\n\n    promo = 0.0\n    promo_block = sp.select_one(\"p.price ins bdi\")\n    if promo_block:\n        promo = parse_price_any(promo_block.get_text(\" \", strip=True))\n\n    out[\"price\"] = float(price or 0.0)\n    out[\"promo_price\"] = float(promo or 0.0)\n    out[\"status\"] = \"ok\" if (out[\"price\"] > 0 or out[\"promo_price\"] > 0) else \"no_price\"\n    return out\n\n\n# --- Site-specific scrapers (ported + cleaned from original notebook) ---\n\n_MONEY_RE = re.compile(r\"\\$\\s*([0-9][0-9\\.,]*)\")\n\ndef _to_cop_int(num_str: str) -> int:\n    if not num_str:\n        return 0\n    s = num_str.strip().replace(\".\", \"\").replace(\",\", \"\")\n    try:\n        return int(s)\n    except Exception:\n        return 0\n\n\ndef _extract_prices_from_html(html: str) -> list[int]:\n    vals = []\n    for m in _MONEY_RE.findall(html or \"\"):\n        v = _to_cop_int(m)\n        if v > 0:\n            vals.append(v)\n    return sorted(set(vals))\n\n\nimport re\nimport requests\nfrom urllib.parse import urlparse, parse_qs\n\ndef _extract_dp_tax_percent(html: str) -> int | None:\n    \"\"\"\n    Sinintermediarios uses a DP (dual price) script that declares:\n      dp_tax_percent = '10'\n    We use this to compute member price = no_member_price / (1 + tax_percent/100)\n    \"\"\"\n    if not html:\n        return None\n\n    # Accept: dp_tax_percent = '10'  OR  dp_tax_percent='10'\n    m = re.search(r\"dp_tax_percent\\s*=\\s*['\\\"](\\d{1,2})['\\\"]\", html, flags=re.IGNORECASE)\n    if not m:\n        return None\n\n    try:\n        return int(m.group(1))\n    except Exception:\n        return None\n\n\nimport re\nimport requests\nfrom urllib.parse import urlparse, parse_qs\n\ndef _extract_dp_tax_percent(html: str) -> int | None:\n    \"\"\"\n    Sinintermediarios dual pricing snippet declares something like:\n      dp_tax_percent = '10'\n    We use it to compute:\n      member_price = full_price / (1 + tax/100)\n    \"\"\"\n    if not html:\n        return None\n    m = re.search(r\"dp_tax_percent\\s*=\\s*['\\\"](\\d{1,2})['\\\"]\", html, flags=re.IGNORECASE)\n    if not m:\n        return None\n    try:\n        return int(m.group(1))\n    except Exception:\n        return None\n\n\ndef scrape_sinintermediarios(url: str, site_key: str = \"sinintermediarios\") -> dict:\n    \"\"\"\n    Shopify (sinintermediarios.co)\n\n    Outputs (as floats for compatibility with your pipeline):\n      - out[\"price\"]      = No Miembro (full) in COP pesos\n      - out[\"membership\"] = Miembro in COP pesos (derived via dp_tax_percent, integer math)\n      - out[\"promo_price\"]= 0.0\n      - out[\"image_url\"]  = best-effort variant-specific image URL\n      - out[\"has_stock\"]  = variant availability\n      - out[\"status\"]     = ok / no_price / http_error / exception\n    \"\"\"\n    out = default_result(site_key, url)\n\n    try:\n        session = requests.Session()\n        session.headers.update(DEFAULT_HEADERS)\n\n        # -----------------------------\n        # 1) Parse variant from URL\n        # -----------------------------\n        parsed = urlparse(url)\n        qs = parse_qs(parsed.query)\n        variant_id = qs.get(\"variant\", [None])[0]\n        variant_id = int(variant_id) if variant_id else None\n\n        # -----------------------------\n        # 2) Fetch Shopify product JSON (.js)\n        # -----------------------------\n        base = f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n        js_url = base.rstrip(\"/\") + \".js\"\n\n        r_js = session.get(js_url, timeout=20)\n        out[\"http_status\"] = int(r_js.status_code)\n        if r_js.status_code != 200:\n            out[\"status\"], out[\"error\"] = \"http_error\", f\"js_http_{r_js.status_code}\"\n            return out\n\n        product = r_js.json()\n\n        # pick selected variant\n        selected = None\n        for v in (product.get(\"variants\") or []):\n            if variant_id and int(v.get(\"id\", 0)) == variant_id:\n                selected = v\n                break\n        if not selected:\n            selected = (product.get(\"variants\") or [None])[0]\n\n        if not selected:\n            out[\"status\"], out[\"error\"] = \"no_price\", \"no_variant\"\n            return out\n\n        # Shopify stores price in cents -> COP pesos\n        raw_cents = int(selected.get(\"price\", 0) or 0)\n        full_pesos = raw_cents // 100  # IMPORTANT\n\n        out[\"has_stock\"] = bool(selected.get(\"available\", True))\n        out[\"name_scraped\"] = product.get(\"title\")\n\n        # -----------------------------\n        # 3) Image URL (prefer variant-size image)\n        # -----------------------------\n        image_url = None\n\n        # 3.1 variant featured_image if present\n        fi = selected.get(\"featured_image\")\n        if isinstance(fi, dict):\n            image_url = fi.get(\"src\")\n\n        # 3.2 try match images[] by option2 size (1LB/2LB/5LB encoded in filenames)\n        if not image_url:\n            imgs = product.get(\"images\") or []\n            opt2 = (selected.get(\"option2\") or \"\").lower()  # e.g., \"5 libras (2280g)\"\n            kw = []\n            if \"1 libra\" in opt2:\n                kw = [\"1lb\", \"1-lb\", \"1_lb\"]\n            elif \"2 libras\" in opt2:\n                kw = [\"2lb\", \"2-lb\", \"2_lb\"]\n            elif \"5 libras\" in opt2:\n                kw = [\"5lb\", \"5-lb\", \"5_lb\"]\n\n            if kw and imgs:\n                for im in imgs:\n                    s = (im or \"\").lower()\n                    if any(k in s for k in kw):\n                        image_url = im\n                        break\n\n        # 3.3 fallback to featured_image\n        if not image_url:\n            image_url = product.get(\"featured_image\")\n\n        if image_url and isinstance(image_url, str) and image_url.startswith(\"//\"):\n            image_url = \"https:\" + image_url\n\n        out[\"image_url\"] = image_url\n\n        # -----------------------------\n        # 4) Membership price (Miembro)\n        #    Derived from dp_tax_percent in HTML using integer math (NO FLOATS)\n        # -----------------------------\n        membership_pesos = 0\n        tax_percent = None\n\n        try:\n            r_html = session.get(url, timeout=20)\n            if r_html.status_code == 200:\n                tax_percent = _extract_dp_tax_percent(r_html.text)\n        except Exception:\n            tax_percent = None\n\n        if tax_percent and tax_percent > 0 and full_pesos > 0:\n            # exact integer math:\n            # member = full * 100 / (100 + tax)\n            denom = 100 + int(tax_percent)\n            membership_pesos = (full_pesos * 100) // denom\n\n        # -----------------------------\n        # 5) Output contract\n        # -----------------------------\n        out[\"price\"] = float(full_pesos)                 # No Miembro\n        out[\"membership\"] = float(membership_pesos)      # Miembro\n        out[\"promo_price\"] = 0.0\n\n        out[\"status\"] = \"ok\" if (full_pesos > 0 or membership_pesos > 0) else \"no_price\"\n        return out\n\n    except Exception as e:\n        out[\"status\"], out[\"error\"] = \"exception\", f\"{type(e).__name__}: {e}\"\n        return out\n\ndef _extract_first_text(html: str, pattern: str):\n    \"\"\"Small helper: returns first regex group match or None.\"\"\"\n    import re\n    try:\n        m = re.search(pattern, html, flags=re.IGNORECASE | re.DOTALL)\n        return m.group(1).strip() if m else None\n    except Exception:\n        return None\n\n#####FARMATODO#########\n\n# # --- Farmatodo (API-first) ---\n\n# ##### FARMATODO (API seed URL strategy; compatible with your HttpClient) #####\n\nimport re\nimport json\nfrom urllib.parse import urlparse, parse_qs, urlencode, urlunparse\n\n# 1) Pega aquí TU URL completa (la del curl) tal cual (sin el \"curl ...\", solo la URL)\nFARMATODO_GETITEM_SEED_URL = (\n    \"https://gw-backend.farmatodo.com/_ah/api/productEndpoint/v2/getItem\"\n    \"?source=WEB&idItem=269800668\"\n    \"&idCustomerWebSafe=ahZzfnN0dW5uaW5nLWJhc2UtMTY0NDAyci4LEgRVc2VyIiQ3ODY4NDA0OC1mOTVmLTRkMjktODliMy0yMzlmYmRmYTFhYTIM\"\n    \"&idStoreGroup=26\"\n    \"&nearbyStores=26,20,67,3,85,24,31,88,81,83,89,15,54\"\n    \"&token=01e0d8b6fd85ad7a5c7120f56b901480\"\n    \"&tokenIdWebSafe=ahZzfnN0dW5uaW5nLWJhc2UtMTY0NDAycl0LEgRVc2VyIiQ3ODY4NDA0OC1mOTVmLTRkMjktODliMy0yMzlmYmRmYTFhYTIMCxIFVG9rZW4iJDRjNTM1YjgyLWU2NDQtNDcxNS1hMWQyLWEzZjgyYjE2MDYzMAw\"\n    \"&key=AIzaSyAidR6Tt0K60gACR78aWThMQb7L5u6Wpag\"\n    \"&deliveryType=EXPRESS&storeId=26&city=BOG&isShoppingCart=false&customerId=undefined\"\n)\n\ndef _extract_farmatodo_item_id(url: str) -> str | None:\n    m = re.search(r\"/producto/(\\d+)\", url)\n    return m.group(1) if m else None\n\ndef _replace_query_param(url: str, key: str, value: str) -> str:\n    p = urlparse(url)\n    q = parse_qs(p.query)\n    q[key] = [value]\n    new_query = urlencode(q, doseq=True)\n    return urlunparse((p.scheme, p.netloc, p.path, p.params, new_query, p.fragment))\n\ndef _pick_product_by_id(getitem_json: dict, id_item: str) -> dict | None:\n    item_sections = getitem_json.get(\"itemSection\") or []\n    for sec in item_sections:\n        for lst in (sec.get(\"list\") or []):\n            for prod in (lst.get(\"product\") or []):\n                if str(prod.get(\"id\")) == str(id_item):\n                    return prod\n    return None\n\ndef _norm_img(url: str | None) -> str | None:\n    if not url:\n        return None\n    if url.startswith(\"//\"):\n        return \"https:\" + url\n    return url\n\ndef scrape_farmatodo(url: str, site_key: str = \"farmatodo\") -> dict:\n    out = default_result(site_key, url)\n\n    try:\n        id_item = _extract_farmatodo_item_id(url)\n        if not id_item:\n            out[\"status\"], out[\"error\"] = \"invalid_input\", \"missing_idItem_in_url\"\n            return out\n\n        # Build API URL from seed (swap idItem only)\n        getitem_url = _replace_query_param(FARMATODO_GETITEM_SEED_URL, \"idItem\", id_item)\n\n        # --- TEMPORARY session headers (since HttpClient.get() doesn't accept headers) ---\n        old_headers = dict(HTTP.session.headers)\n\n        # Minimal headers that commonly matter for this endpoint\n        HTTP.session.headers.update({\n            \"Accept\": \"application/json, text/plain, */*\",\n            \"Content-Type\": \"application/json\",\n            \"source\": \"WEB\",\n            \"country\": \"COL\",\n            \"device-id\": \"ANONIMO\",\n            # Referer/origin sometimes help; keep them stable\n            \"origin\": \"https://www.farmatodo.com.co\",\n            \"referer\": url,\n        })\n\n        fr_api = HTTP.get(getitem_url)\n\n        # restore headers no matter what\n        HTTP.session.headers.clear()\n        HTTP.session.headers.update(old_headers)\n\n        out[\"http_status\"], out[\"elapsed_ms\"] = fr_api.status_code, fr_api.elapsed_ms\n\n        if fr_api.error:\n            out[\"status\"], out[\"error\"] = \"exception\", fr_api.error\n            return out\n        if fr_api.status_code >= 400:\n            out[\"status\"], out[\"error\"] = \"http_error\", f\"getitem_http_{fr_api.status_code}\"\n            return out\n\n        try:\n            data = json.loads((fr_api.text or \"\").strip())\n        except Exception as e:\n            out[\"status\"], out[\"error\"] = \"exception\", f\"failed_to_parse_getitem_json: {e}\"\n            return out\n\n        prod = _pick_product_by_id(data, id_item)\n        if not prod:\n            # If we didn't find the id, don't accept a random fallback product (this is the \"same price for all\" symptom)\n            out[\"status\"] = \"no_price\"\n            out[\"error\"] = \"getitem_id_not_found_in_response\"\n            return out\n\n        # API returns numeric COP values (pesos)\n        full_price  = int(float(prod.get(\"fullPrice\") or 0))\n        offer_price = int(float(prod.get(\"offerPrice\") or 0))\n        prime_price = int(float(prod.get(\"primePrice\") or 0))\n\n        promo = offer_price if (offer_price > 0 and full_price > 0 and offer_price < full_price) else 0\n        membership = prime_price if prime_price > 0 else 0\n\n        imgs = prod.get(\"listUrlImages\") or []\n        image_url = _norm_img(imgs[0]) if imgs else _norm_img(prod.get(\"mediaImageUrl\"))\n\n        out[\"price\"] = float(full_price or 0)\n        out[\"promo_price\"] = float(promo or 0)\n        out[\"membership\"] = float(membership or 0)\n\n        out[\"name_scraped\"] = prod.get(\"description\") or prod.get(\"name\")\n        out[\"image_url\"] = image_url\n        out[\"image\"] = image_url or \"\"\n\n        out[\"has_stock\"] = bool(prod.get(\"hasStock\", True))\n        out[\"stock\"] = 0\n\n        out[\"status\"] = \"ok\" if (out[\"price\"] > 0 or out[\"promo_price\"] > 0 or out[\"membership\"] > 0) else \"no_price\"\n        return out\n\n    except Exception as e:\n        out[\"status\"], out[\"error\"] = \"exception\", f\"{type(e).__name__}: {e}\"\n        return out\n\n\n\n# --- Muscletech (WooCommerce / WordPress) ---\n\ndef parse_price_cop_thousands(x) -> float:\n    \"\"\"\n    COP helper:\n    - \"160.650\" -> 160650\n    - \"$ 178.500\" -> 178500\n    - \"20780000\" (rare) -> 20780000\n    \"\"\"\n    if x is None:\n        return 0.0\n    s = str(x)\n    s = s.replace(\"\\xa0\", \" \").strip()\n\n    # keep digits + separators\n    s_clean = re.sub(r\"[^\\d\\.,]\", \"\", s_clean := s)\n\n    if not s_clean:\n        return 0.0\n\n    # If contains BOTH, use your existing heuristic\n    if \",\" in s_clean and \".\" in s_clean:\n        # Example: \"1.234,56\" or \"1,234.56\"\n        if s_clean.find(\",\") < s_clean.find(\".\"):\n            s_clean = s_clean.replace(\",\", \"\")\n        else:\n            s_clean = s_clean.replace(\".\", \"\").replace(\",\", \".\")\n        try:\n            return float(s_clean)\n        except:\n            return 0.0\n\n    # Only comma\n    if \",\" in s_clean and \".\" not in s_clean:\n        # if looks like thousands \"123,456\" => remove commas\n        if re.search(r\",\\d{3}($|[^\\d])\", s_clean):\n            s_clean = s_clean.replace(\",\", \"\")\n        else:\n            s_clean = s_clean.replace(\",\", \".\")\n        try:\n            return float(s_clean)\n        except:\n            return 0.0\n\n    # Only dot\n    if \".\" in s_clean and \",\" not in s_clean:\n        # if looks like thousands \"160.650\" or \"1.234.567\" => remove dots\n        if re.search(r\"\\.\\d{3}($|[^\\d])\", s_clean) or s_clean.count(\".\") >= 2:\n            s_clean = s_clean.replace(\".\", \"\")\n            try:\n                return float(s_clean)\n            except:\n                return 0.0\n\n        # otherwise treat dot as decimal\n        try:\n            return float(s_clean)\n        except:\n            return 0.0\n\n    # Only digits\n    try:\n        return float(s_clean)\n    except:\n        return 0.0\n\n\ndef scrape_muscletech(url: str, site_key: str = \"muscletech\") -> dict:\n    out = default_result(site_key, url)\n\n    fr = HTTP.get(url)\n    out[\"http_status\"], out[\"elapsed_ms\"] = fr.status_code, fr.elapsed_ms\n\n    if fr.error:\n        out[\"status\"], out[\"error\"] = \"exception\", fr.error\n        return out\n    if fr.status_code >= 400:\n        out[\"status\"], out[\"error\"] = \"http_error\", f\"http_status={fr.status_code}\"\n        return out\n\n    html = fr.text\n    sp = soup(html)\n\n    out[\"image_url\"] = extract_og_image_url(sp)\n\n    # WooCommerce usual pattern:\n    # <p class=\"price\"><del>...178.500...</del> <ins>...160.650...</ins></p>\n    sale_el = sp.select_one(\"p.price ins .woocommerce-Price-amount, p.price ins bdi\")\n    reg_el  = sp.select_one(\"p.price del .woocommerce-Price-amount, p.price del bdi\")\n\n    sale_txt = sale_el.get_text(\" \", strip=True) if sale_el else \"\"\n    reg_txt  = reg_el.get_text(\" \", strip=True) if reg_el else \"\"\n\n    sale_price = parse_price_cop_thousands(sale_txt)\n    reg_price  = parse_price_cop_thousands(reg_txt)\n\n    # fallback: sometimes there's only one visible price\n    if reg_price <= 0:\n        any_price_el = sp.select_one(\"p.price .woocommerce-Price-amount, p.price bdi\")\n        any_txt = any_price_el.get_text(\" \", strip=True) if any_price_el else \"\"\n        reg_price = parse_price_cop_thousands(any_txt)\n\n    # If sale exists but reg missing, treat reg= sale\n    if reg_price <= 0 and sale_price > 0:\n        reg_price = sale_price\n\n    # stock heuristic\n    body_classes = (sp.body.get(\"class\", []) if sp.body else [])\n    add_btn = sp.select_one(\"button.single_add_to_cart_button\")\n    out[\"has_stock\"] = bool(add_btn) and (\"outofstock\" not in body_classes)\n\n    out[\"price\"] = float(reg_price or 0.0)          # full/regular\n    out[\"promo_price\"] = float(sale_price or 0.0)   # discount/sale\n    out[\"membership\"] = 0.0\n\n    out[\"status\"] = \"ok\" if (out[\"price\"] > 0 or out[\"promo_price\"] > 0) else \"no_price\"\n    return out\n\n\n# --- Savvy (FIX: COP thousands parsing + subscription membership) ---\n\ndef parse_price_cop_strict(text: str) -> float:\n    \"\"\"\n    COP parser:\n    - '$207.800' => 207800\n    - '$166.240' => 166240\n    - '$207,800' => 207800\n    - '207800'   => 207800\n    \"\"\"\n    if not text:\n        return 0.0\n\n    s = str(text)\n    # keep digits + separators only\n    s = re.sub(r\"[^\\d\\.,]\", \"\", s).strip()\n    if not s:\n        return 0.0\n\n    # If both separators exist, assume:\n    # - '.' thousands and ',' decimals OR vice versa. For COP we want integer pesos.\n    if \".\" in s and \",\" in s:\n        # choose last separator as decimal separator; remove the other as thousands\n        if s.rfind(\".\") > s.rfind(\",\"):\n            # '.' is decimal, ',' thousands\n            s = s.replace(\",\", \"\")\n        else:\n            # ',' is decimal, '.' thousands\n            s = s.replace(\".\", \"\").replace(\",\", \".\")\n        try:\n            return float(int(float(s)))\n        except:\n            return 0.0\n\n    # Only dot: treat as thousands separator (COP formatting)\n    if \".\" in s and \",\" not in s:\n        s2 = s.replace(\".\", \"\")\n        return float(int(s2)) if s2.isdigit() else 0.0\n\n    # Only comma: could be thousands or decimal; for COP usually thousands in LATAM UI\n    if \",\" in s and \".\" not in s:\n        s2 = s.replace(\",\", \"\")\n        return float(int(s2)) if s2.isdigit() else 0.0\n\n    # plain digits\n    return float(int(s)) if s.isdigit() else 0.0\n\n\ndef _first_price_by_ids_cop(sp: BeautifulSoup, selectors: list[str]) -> float:\n    for sel in selectors:\n        el = sp.select_one(sel)\n        if el:\n            p = parse_price_cop_strict(el.get_text(\" \", strip=True))\n            if p > 0:\n                return p\n    return 0.0\n\n\ndef scrape_savvy(url: str, site_key: str = \"savvy\") -> dict:\n    \"\"\"\n    Savvy is Shopify.\n    - price_full: from product.js (cents -> COP pesos)\n    - membership: subscription price (if present) from HTML IDs\n    \"\"\"\n    out = default_result(site_key, url)\n\n    try:\n        session = requests.Session()\n        session.headers.update(DEFAULT_HEADERS)\n\n        # 1) Always pull product.js (truth for full price in cents)\n        parsed = urlparse(url)\n        base = f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n        js_url = base.rstrip(\"/\") + \".js\"\n\n        r_js = session.get(js_url, timeout=20)\n        out[\"http_status\"] = int(r_js.status_code)\n        if r_js.status_code != 200:\n            out[\"status\"], out[\"error\"] = \"http_error\", f\"js_http_{r_js.status_code}\"\n            return out\n\n        product = r_js.json()\n\n        # pick first variant (Savvy often uses Default Title)\n        variants = product.get(\"variants\") or []\n        v = variants[0] if variants else None\n        if not v:\n            out[\"status\"], out[\"error\"] = \"no_price\", \"no_variant\"\n            return out\n\n        # Shopify stores price in cents -> COP pesos\n        price_full = int(v.get(\"price\", 0)) // 100\n        out[\"has_stock\"] = bool(v.get(\"available\", True))\n\n        # image\n        img = product.get(\"featured_image\")\n        if isinstance(img, str) and img.startswith(\"//\"):\n            img = \"https:\" + img\n        out[\"image_url\"] = img\n\n        # 2) HTML to get subscription (membership) price (if shown)\n        r_html = session.get(url, timeout=20)\n        if r_html.status_code == 200:\n            sp = soup(r_html.text)\n\n            # these ids exist in your SAVVYHTML (confirmed)\n            membership = _first_price_by_ids_cop(sp, [\n                \"#savvy-prime-precio-final-product-page\",\n                \"#savvy-prime-precio-final1-product-page\",\n                \"#savvy-prime-precio-final2-product-page\",\n                \"#savvy-prime-precio-final3-product-page\",\n            ])\n\n            # fallback: infer from tokens inside the same container\n            if membership <= 0:\n                block = re.search(\n                    r'id=\"precios-savvy-prime-container-product-page\".{0,8000}?</div>',\n                    r_html.text, re.IGNORECASE | re.DOTALL\n                )\n                html_block = block.group(0) if block else r_html.text\n                tokens = re.findall(r\"\\$\\s*[\\d\\.\\,]+\", html_block)\n                vals = sorted({int(parse_price_cop_strict(t)) for t in tokens if parse_price_cop_strict(t) >= 10000})\n                if len(vals) >= 2:\n                    membership = float(min(vals))\n\n            out[\"membership\"] = float(membership or 0.0)\n        else:\n            out[\"membership\"] = 0.0\n\n        out[\"price\"] = float(price_full or 0.0)\n        out[\"status\"] = \"ok\" if (out[\"price\"] > 0 or out[\"membership\"] > 0) else \"no_price\"\n        return out\n\n    except Exception as e:\n        out[\"status\"], out[\"error\"] = \"exception\", f\"{type(e).__name__}: {e}\"\n        return out\n\n\n# --- ZonaFit (Shopify product .json) ---\n\ndef _get_variant_id_from_url(url: str) -> str:\n    try:\n        q = parse_qs(urlparse(url).query)\n        return q.get(\"variant\", [\"\"])[0]\n    except Exception:\n        return \"\"\n\n\ndef scrape_zonafit(url: str, site_key: str = \"zonafit\") -> dict:\n    out = default_result(site_key, url)\n\n    fr = HTTP.get(url)\n    out[\"http_status\"], out[\"elapsed_ms\"] = fr.status_code, fr.elapsed_ms\n    if fr.error:\n        out[\"status\"], out[\"error\"] = \"exception\", fr.error\n        return out\n    if fr.status_code >= 400:\n        out[\"status\"], out[\"error\"] = \"http_error\", f\"http_status={fr.status_code}\"\n        return out\n\n    sp = soup(fr.text)\n    og_img = extract_og_image_url(sp)\n\n    base = url.split(\"?\")[0].rstrip(\"/\")\n    json_url = base + \".json\"\n\n    frj = HTTP.get(json_url)\n    if frj.error or frj.status_code >= 400:\n        out[\"image_url\"] = og_img\n        out[\"status\"] = \"no_price\"\n        out[\"error\"] = f\"shopify_json_http_status={frj.status_code}\"\n        return out\n\n    try:\n        data = json.loads((frj.text or \"\").strip())\n    except Exception as e:\n        out[\"status\"], out[\"error\"] = \"exception\", f\"failed_to_parse_shopify_json: {e}\"\n        return out\n\n    product = data.get(\"product\") or {}\n    variants = product.get(\"variants\") or []\n    images = product.get(\"images\") or []\n\n    variant_id = _get_variant_id_from_url(url)\n    selected = None\n    for v in variants:\n        if variant_id and str(v.get(\"id\")) == str(variant_id):\n            selected = v\n            break\n    selected = selected or (variants[0] if variants else None)\n\n    out[\"price\"] = parse_price_any((selected or {}).get(\"price\"))\n\n    # image selection\n    image_url = None\n    img_obj = (selected or {}).get(\"featured_image\")\n    if isinstance(img_obj, dict):\n        image_url = img_obj.get(\"src\")\n    elif isinstance(img_obj, str):\n        image_url = img_obj\n\n    if not image_url and images:\n        image_url = images[0] if isinstance(images[0], str) else images[0].get(\"src\")\n\n    out[\"image_url\"] = image_url or og_img\n    out[\"status\"] = \"ok\" if out[\"price\"] > 0 else \"no_price\"\n    return out\n\n\n# --- Vitanas / Proscience / Nutramerican / Colsubsidio / Farmatodo / Herbivore ---\n# Note: These were copied as-is from original notebook with only parse function normalization.\n\n# (To keep this response within reasonable size, we keep the remaining scrapers close to original.)\n\n\ndef scrape_vitanas(url: str) -> dict:\n    \"\"\"\n    Vitanas is WooCommerce, but:\n      - price comes like \"$158.000\" and current generic logic is parsing as 158.0\n      - image is in <picture><source srcset=...> (often webp)\n    So we force price parsing via parse_price_cop() and extract image from og:image / picture / img.\n    \"\"\"\n    r = HTTP.get(url)\n\n    out = {\n        \"status\": \"no_price\",\n        \"error\": r.error,\n        \"http_status\": r.status_code,\n        \"elapsed_ms\": r.elapsed_ms,\n        \"price\": 0,\n        \"promo_price\": 0,\n        \"membership\": 0,\n        \"has_stock\": True,\n        \"stock\": 0,\n        \"image\": \"\",\n        \"image_url\": \"\",\n    }\n\n    if r.status_code != 200 or not r.text:\n        out[\"status\"] = \"exception\" if out[\"error\"] else \"http_error\"\n        return out\n\n    soup = BeautifulSoup(r.text, \"html.parser\")\n\n    # --- PRICE (COP) ---\n    # Same root-cause fix as proscience: target single <bdi> not the container.\n    price_text = \"\"\n\n    meta_price = soup.select_one('meta[property=\"product:price:amount\"]')\n    if meta_price and meta_price.get(\"content\"):\n        price_text = meta_price.get(\"content\", \"\")\n\n    if not price_text:\n        el = (\n            soup.select_one(\"p.price del .woocommerce-Price-amount bdi\") or\n            soup.select_one(\"p.price del bdi\") or\n            soup.select_one(\"p.price .woocommerce-Price-amount bdi\") or\n            soup.select_one(\".summary .price .woocommerce-Price-amount bdi\") or\n            soup.select_one(\"span.woocommerce-Price-amount bdi\")\n        )\n        if el:\n            price_text = el.get_text(\" \", strip=True)\n\n    price_cop = int(parse_price_cop(price_text) or 0)\n    out[\"price\"] = price_cop\n\n    # --- IMAGE ---\n    # 1) og:image\n    og = soup.select_one('meta[property=\"og:image\"], meta[name=\"og:image\"]')\n    if og and og.get(\"content\"):\n        out[\"image_url\"] = og.get(\"content\", \"\").strip()\n\n    # 2) picture source srcset (matches what you showed in DevTools)\n    if not out[\"image_url\"]:\n        src = soup.select_one(\"picture source[srcset]\")\n        if src and src.get(\"srcset\"):\n            # Parse first URL from srcset (inline — _first_url_from_srcset was never defined)\n            out[\"image_url\"] = src.get(\"srcset\", \"\").split(\",\")[0].strip().split()[0]\n\n    # 3) fallback: main product image\n    if not out[\"image_url\"]:\n        img = soup.select_one(\"img.wp-post-image, .woocommerce-product-gallery__image img, img.attachment-shop_single\")\n        if img and img.get(\"src\"):\n            out[\"image_url\"] = img.get(\"src\", \"\").strip()\n\n    out[\"status\"] = \"ok\" if out[\"price\"] > 0 else \"no_price\"\n    return out\n\n####################!!!!!!!PROSCIENCE!!!!!!!################\n\ndef scrape_proscience(url: str) -> dict:\n    \"\"\"\n    Proscience pages are reachable by scraper even if browser blocks you.\n    Issue observed: price comes as 159.9 / 350 instead of 159900 / 350000.\n    Fix: force parse_price_cop() from common WooCommerce nodes.\n    \"\"\"\n    r = HTTP.get(url)\n\n    out = {\n        \"status\": \"no_price\",\n        \"error\": r.error,\n        \"http_status\": r.status_code,\n        \"elapsed_ms\": r.elapsed_ms,\n        \"price\": 0,\n        \"promo_price\": 0,\n        \"membership\": 0,\n        \"has_stock\": True,\n        \"stock\": 0,\n        \"image\": \"\",\n        \"image_url\": \"\",\n    }\n\n    if r.status_code != 200 or not r.text:\n        out[\"status\"] = \"exception\" if out[\"error\"] else \"http_error\"\n        return out\n\n    soup = BeautifulSoup(r.text, \"html.parser\")\n\n    # PRICE\n    # Root-cause fix: the old selector (\"p.price\") grabbed the entire WooCommerce\n    # price container, which can hold BOTH the regular and sale <bdi> elements.\n    # get_text() on that container concatenates all amounts into one giant string\n    # (e.g. \"214900214900171920171920\"), causing OverflowError in Spark.\n    # Fix: target the single <bdi> element for regular price, then separately\n    # extract the sale price into promo_price.\n    price_text = \"\"\n\n    meta_price = soup.select_one('meta[property=\"product:price:amount\"]')\n    if meta_price and meta_price.get(\"content\"):\n        price_text = meta_price.get(\"content\", \"\")\n\n    if not price_text:\n        # Prefer del (full/regular price), fall back to first bdi in p.price\n        el = (\n            soup.select_one(\"p.price del .woocommerce-Price-amount bdi\") or\n            soup.select_one(\"p.price del bdi\") or\n            soup.select_one(\"p.price .woocommerce-Price-amount bdi\") or\n            soup.select_one(\".summary .price .woocommerce-Price-amount bdi\") or\n            soup.select_one(\"span.woocommerce-Price-amount bdi\")\n        )\n        if el:\n            price_text = el.get_text(\" \", strip=True)\n\n    # Promo price (ins = sale price)\n    promo_el = (\n        soup.select_one(\"p.price ins .woocommerce-Price-amount bdi\") or\n        soup.select_one(\"p.price ins bdi\")\n    )\n    if promo_el:\n        out[\"promo_price\"] = int(parse_price_cop(promo_el.get_text(\" \", strip=True)) or 0)\n\n    out[\"price\"] = int(parse_price_cop(price_text) or 0)\n\n    # IMAGE\n    og = soup.select_one('meta[property=\"og:image\"], meta[name=\"og:image\"]')\n    if og and og.get(\"content\"):\n        out[\"image_url\"] = og.get(\"content\", \"\").strip()\n\n    if not out[\"image_url\"]:\n        img = soup.select_one(\"img.wp-post-image, .woocommerce-product-gallery__image img, img.attachment-shop_single\")\n        if img and img.get(\"src\"):\n            out[\"image_url\"] = img.get(\"src\", \"\").strip()\n\n    out[\"status\"] = \"ok\" if out[\"price\"] > 0 else \"no_price\"\n    return out\n\n####################!!!!!!!NUTRAMERICAN!!!!!!!################\n\ndef scrape_nutramerican(url: str, site_key: str = \"nutramerican\") -> dict:\n    t0 = time.time()\n\n    out = {\n        \"status\": \"no_price\",\n        \"error\": None,\n        \"site_raw\": site_key,\n        \"site\": site_key,\n        \"url\": url,\n\n        \"price\": 0,\n        \"promo_price\": 0,\n        \"membership\": 0,\n\n        \"has_stock\": True,\n        \"stock\": 0,\n        \"image\": \"\",\n        \"image_url\": \"\",\n\n        \"http_status\": 0,\n        \"elapsed_ms\": 0,\n    }\n\n    try:\n        headers = {\n            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n            \"Accept-Language\": \"es-CO,es;q=0.9,en;q=0.8\",\n            \"Connection\": \"keep-alive\",\n        }\n\n        r = requests.get(url, headers=headers, timeout=20, allow_redirects=True)\n        out[\"http_status\"] = r.status_code\n        out[\"elapsed_ms\"] = int((time.time() - t0) * 1000)\n\n        if r.status_code != 200 or not r.text:\n            out[\"status\"] = \"http_error\"\n            out[\"error\"] = f\"http_status={r.status_code}\"\n            return out\n\n        soup = BeautifulSoup(r.text, \"html.parser\")\n\n        # -----------------------\n        # PRICE (IMPORTANT FIX)\n        # -----------------------\n        # Nutramerican has:\n        #   - s#priceDefaultDes  -> $61.99 (USD, hidden)\n        #   - strong#priceDefault -> $224,990 (COP, visible)  <-- use this\n        price_txt = \"\"\n        price_el = soup.select_one(\"strong#priceDefault\")\n        if price_el:\n            price_txt = price_el.get_text(\" \", strip=True)\n\n        # fallback (ONLY if priceDefault missing)\n        if not price_txt:\n            # try discount as fallback\n            disc_el = soup.select_one(\"strong#priceDiscount\")\n            if disc_el:\n                price_txt = disc_el.get_text(\" \", strip=True)\n\n        price = parse_price_cop(price_txt) if price_txt else 0\n        out[\"price\"] = int(price) if price else 0\n\n        # -----------------------\n        # PROMO PRICE (optional)\n        # -----------------------\n        promo_txt = \"\"\n        promo_el = soup.select_one(\"strong#priceDiscount\")\n        if promo_el:\n            promo_txt = promo_el.get_text(\" \", strip=True)\n        promo = parse_price_cop(promo_txt) if promo_txt else 0\n        out[\"promo_price\"] = int(promo) if promo else 0\n\n        # -----------------------\n        # IMAGE\n        # -----------------------\n        # Prefer og:image\n        og_img = soup.select_one(\"meta[property='og:image']\")\n        if og_img and og_img.get(\"content\"):\n            out[\"image_url\"] = og_img[\"content\"].strip()\n        else:\n            # fallback: first product img if present\n            img = soup.select_one(\"article img\")\n            if img and img.get(\"src\"):\n                out[\"image_url\"] = img[\"src\"].strip()\n\n        # final status\n        out[\"status\"] = \"ok\" if out[\"price\"] > 0 else \"no_price\"\n        return out\n\n    except Exception as e:\n        out[\"status\"] = \"exception\"\n        out[\"error\"] = f\"{type(e).__name__}: {e}\"\n        out[\"elapsed_ms\"] = int((time.time() - t0) * 1000)\n        return out\n\n####################!!!!!!!COLSUBSIDIO!!!!!!!################\n\ndef scrape_colsubsidio(url: str, site_key: str):\n    t0 = time.time()\n\n    out = {\n        \"status\": \"no_price\",\n        \"error\": None,\n        \"http_status\": 0,\n        \"elapsed_ms\": 0,\n        \"price\": 0,\n        \"promo_price\": 0,\n        \"membership\": 0,\n        \"has_stock\": True,\n        \"stock\": 0,\n        \"image\": \"\",\n        \"image_url\": None,\n    }\n\n    try:\n        resp = HTTP.get(url)  # no timeout=\n        out[\"http_status\"] = getattr(resp, \"status_code\", 0)\n        html = getattr(resp, \"text\", \"\") or \"\"\n        out[\"elapsed_ms\"] = int((time.time() - t0) * 1000)\n\n        if out[\"http_status\"] != 200 or not html:\n            out[\"status\"] = \"exception\"\n            out[\"error\"] = f\"http_status={out['http_status']}\"\n            return out\n\n        soup = BeautifulSoup(html, \"html.parser\")\n\n        # --- IMAGE (already working for you, keep it)\n        og_img = soup.find(\"meta\", attrs={\"property\": \"og:image\"})\n        if og_img and og_img.get(\"content\"):\n            out[\"image_url\"] = og_img[\"content\"]\n        else:\n            img = soup.select_one('img[src*=\"vtexassets.com/arquivos/ids\"]')\n            if img and img.get(\"src\"):\n                out[\"image_url\"] = img[\"src\"]\n\n        # --- PRICE attempt 1: VTEX DOM spans (works only if SSR includes price)\n        int_el = soup.select_one(\".vtex-product-price-1-x-currencyInteger\")\n        dec_el = soup.select_one(\".vtex-product-price-1-x-currencyDecimal\")\n\n        int_txt = int_el.get_text(strip=True) if int_el else \"\"\n        dec_txt = dec_el.get_text(strip=True) if dec_el else \"\"\n\n        int_digits = re.sub(r\"\\D\", \"\", int_txt)\n        dec_digits = re.sub(r\"\\D\", \"\", dec_txt)\n\n        if int_digits and dec_digits:\n            dec_digits = (dec_digits + \"000\")[:3]  # 226 + 750 => 226750\n            candidate = int(int_digits + dec_digits)\n            # Guard: if int_el captured the full formatted price (e.g. \"2.267.500\"),\n            # int_digits already has all digits and the concat produces a 9-digit number\n            # that overflows IntegerType and gives a nonsense price. Cap at 20M COP\n            # and fall through to the JSON fallback if exceeded.\n            if candidate <= 20_000_000:\n                out[\"price\"] = candidate\n\n        # --- PRICE attempt 2: fallback to embedded VTEX JSON in HTML\n        # Many VTEX pages embed commertialOffer with keys like:\n        # \"sellingPrice\":226750 or \"Price\":226750 or \"spotPrice\":226750\n        if out[\"price\"] <= 0:\n            candidates = []\n\n            # Try common numeric fields in VTEX state\n            patterns = [\n                r'\"sellingPrice\"\\s*:\\s*([0-9]+(?:\\.[0-9]+)?)',\n                r'\"spotPrice\"\\s*:\\s*([0-9]+(?:\\.[0-9]+)?)',\n                r'\"Price\"\\s*:\\s*([0-9]+(?:\\.[0-9]+)?)',\n                r'\"ListPrice\"\\s*:\\s*([0-9]+(?:\\.[0-9]+)?)',\n                r'\"price\"\\s*:\\s*([0-9]+(?:\\.[0-9]+)?)',\n            ]\n\n            for p in patterns:\n                for m in re.finditer(p, html):\n                    try:\n                        v = float(m.group(1))\n                        # filter crazy small values (e.g., 61) and keep COP-like numbers\n                        if v >= 1000:\n                            candidates.append(int(round(v)))\n                    except Exception:\n                        pass\n\n            # choose best candidate:\n            # - In VTEX, selling price typically appears many times; pick the mode-ish by using max of reasonable values\n            # - safer than min (min sometimes finds grams or unrelated small fields)\n            candidates = [c for c in candidates if 1000 <= c <= 20000000]\n            if candidates:\n                out[\"price\"] = max(candidates)\n\n        # --- PRICE attempt 3: last fallback text \"$226.750\" if present\n        if out[\"price\"] <= 0:\n            m = re.search(r\"\\$\\s*[\\d\\.\\,]+\", soup.get_text(\" \", strip=True))\n            if m:\n                out[\"price\"] = int(parse_price_cop(m.group(0)))\n\n        out[\"status\"] = \"ok\" if out[\"price\"] > 0 else \"no_price\"\n        return out\n\n    except Exception as e:\n        out[\"status\"] = \"exception\"\n        out[\"error\"] = repr(e)\n        out[\"elapsed_ms\"] = int((time.time() - t0) * 1000)\n        return out\n\n\n\n####################!!!!!!!HERVIBORE!!!!!!!################\n\ndef scrape_herbivore(url: str, site_key: str):\n    t0 = time.time()\n\n    out = {\n        \"status\": \"no_price\",\n        \"error\": None,\n        \"http_status\": 0,\n        \"elapsed_ms\": 0,\n        \"price\": 0,\n        \"promo_price\": 0,\n        \"membership\": 0,\n        \"has_stock\": True,\n        \"stock\": 0,\n        \"image\": \"\",\n        \"image_url\": None,\n    }\n\n    try:\n        resp = HTTP.get(url)  # no timeout=\n        out[\"http_status\"] = getattr(resp, \"status_code\", 0)\n        html = getattr(resp, \"text\", \"\") or \"\"\n        out[\"elapsed_ms\"] = int((time.time() - t0) * 1000)\n\n        if out[\"http_status\"] != 200 or not html:\n            out[\"status\"] = \"exception\"\n            out[\"error\"] = f\"http_status={out['http_status']}\"\n            return out\n\n        soup = BeautifulSoup(html, \"html.parser\")\n\n        # --- PRICE: use the formatted on-page price string like \"$ 199.000\"\n        # In your screenshot it sits in:\n        # span.woocommerce-Price-amount amount > bdi\n        bdi = soup.select_one(\"span.woocommerce-Price-amount.amount bdi\")\n        if bdi:\n            price_txt = bdi.get_text(\" \", strip=True)\n            out[\"price\"] = int(parse_price_cop(price_txt))\n\n        # fallback: any $xxx.xxx on page\n        if out[\"price\"] <= 0:\n            m = re.search(r\"\\$\\s*[\\d\\.\\,]+\", soup.get_text(\" \", strip=True))\n            if m:\n                out[\"price\"] = int(parse_price_cop(m.group(0)))\n\n        # --- IMAGE\n        og_img = soup.find(\"meta\", attrs={\"property\": \"og:image\"})\n        if og_img and og_img.get(\"content\"):\n            out[\"image_url\"] = og_img[\"content\"]\n        else:\n            img = soup.select_one(\"img.wp-post-image\") or soup.select_one('img[src*=\"/wp-content/uploads/\"]')\n            if img and img.get(\"src\"):\n                out[\"image_url\"] = img[\"src\"]\n\n        out[\"status\"] = \"ok\" if out[\"price\"] > 0 else \"no_price\"\n        return out\n\n    except Exception as e:\n        out[\"status\"] = \"exception\"\n        out[\"error\"] = repr(e)\n        out[\"elapsed_ms\"] = int((time.time() - t0) * 1000)\n        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed42dec8-b538-4322-be5a-26638699cf4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------\n",
    "# Dispatcher + row-level wrapper\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "SITE_TO_SCRAPER = {\n",
    "    \"savvy\": lambda u: scrape_savvy(u, \"savvy\"),\n",
    "    \"zonafit\": lambda u: scrape_zonafit(u, \"zonafit\"),\n",
    "    \"sinintermediarios\": lambda u: scrape_sinintermediarios(u, \"sinintermediarios\"),\n",
    "\n",
    "    \"proscience\": lambda u: scrape_proscience(u),\n",
    "    \"nutramerican\": lambda u: scrape_nutramerican(u, \"nutramerican\"),\n",
    "    \"vitanas\": lambda u: scrape_vitanas(u),\n",
    "\n",
    "    \"muscletech\": lambda u: scrape_muscletech(u, \"muscletech\"),\n",
    "    \"colsubsidio\": lambda u: scrape_colsubsidio(u, \"colsubsidio\"),\n",
    "    \"herbivore\": lambda u: scrape_herbivore(u, \"herbivore\"),\n",
    "    \"farmatodo\": lambda u: scrape_farmatodo(u, \"farmatodo\"),\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "def standardize_result(site_raw: str, site_key: str, url: str, r: dict) -> dict:\n",
    "    \"\"\"Normalize schema + ensure prices are numeric.\"\"\"\n",
    "    out = default_result(site_key, url)\n",
    "    out.update(r or {})\n",
    "\n",
    "    out[\"site\"] = out.get(\"site\") or site_key\n",
    "    out[\"url\"] = out.get(\"url\") or url\n",
    "\n",
    "    # Standardize timestamps\n",
    "    out[\"scraped_at\"] = datetime.datetime.now(datetime.timezone.utc).isoformat()\n",
    "\n",
    "    # Normalize prices\n",
    "    out[\"price\"] = cop_thousands_fix(parse_price_any(out.get(\"price\")))\n",
    "    out[\"promo_price\"] = cop_thousands_fix(parse_price_any(out.get(\"promo_price\")))\n",
    "    out[\"membership\"] = cop_thousands_fix(parse_price_any(out.get(\"membership\")))\n",
    "\n",
    "    # Derive status when scraper didn't set it (or set ok with no price)\n",
    "    if out.get(\"status\") in (None, \"\", \"ok\"):\n",
    "        has_any = max(out[\"price\"], out[\"promo_price\"], out[\"membership\"]) > 0\n",
    "        out[\"status\"] = \"ok\" if has_any else \"no_price\"\n",
    "\n",
    "    # Attach raw site\n",
    "    out[\"site_raw\"] = site_raw\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def scrape_row(site_raw: str, url_raw: str, request_sleep_s: float = 0.25) -> dict:\n",
    "    site_key = normalize_site(site_raw)\n",
    "    url = clean_url(url_raw)\n",
    "\n",
    "    if not site_key or not url:\n",
    "        r = default_result(site_key or None, url or None)\n",
    "        r.update({\n",
    "            \"site_raw\": site_raw,\n",
    "            \"status\": \"invalid_input\",\n",
    "            \"error\": f\"missing site/url (site_raw='{site_raw}', url_raw='{url_raw}')\",\n",
    "            \"scraped_at\": datetime.datetime.now(datetime.timezone.utc).isoformat(),\n",
    "        })\n",
    "        return r\n",
    "\n",
    "    scraper = SITE_TO_SCRAPER.get(site_key)\n",
    "    if not scraper:\n",
    "        r = default_result(site_key, url)\n",
    "        r.update({\n",
    "            \"site_raw\": site_raw,\n",
    "            \"status\": \"unsupported_site\",\n",
    "            \"error\": f\"unsupported site key '{site_key}' (raw='{site_raw}')\",\n",
    "            \"scraped_at\": datetime.datetime.now(datetime.timezone.utc).isoformat(),\n",
    "        })\n",
    "        return r\n",
    "\n",
    "    try:\n",
    "        raw = scraper(url) or {}\n",
    "        return standardize_result(site_raw, site_key, url, raw)\n",
    "    except Exception as e:\n",
    "        r = default_result(site_key, url)\n",
    "        r.update({\n",
    "            \"site_raw\": site_raw,\n",
    "            \"status\": \"exception\",\n",
    "            \"error\": f\"{type(e).__name__}: {str(e)[:500]}\",\n",
    "            \"scraped_at\": datetime.datetime.now(datetime.timezone.utc).isoformat(),\n",
    "        })\n",
    "        return r\n",
    "    finally:\n",
    "        time.sleep(request_sleep_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94a15b0c-2522-4e63-b2c3-afed567e8557",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# ---- Source table + columns ----\n",
    "MAIN_TABLE = \"workspace.sinintermediarios.main_file\"\n",
    "URL_COL = \"url\"\n",
    "SITE_COL = \"comercio\"   # <-- IMPORTANT: your store column\n",
    "\n",
    "main_df = spark.table(MAIN_TABLE)\n",
    "\n",
    "# ---- Build input dataframe for scraping ----\n",
    "input_df = (\n",
    "    main_df\n",
    "    .select(\n",
    "        F.col(SITE_COL).cast(\"string\").alias(\"site_raw\"),\n",
    "        F.col(URL_COL).cast(\"string\").alias(\"url_raw\"),\n",
    "    )\n",
    "    .where(F.col(\"url_raw\").isNotNull() & (F.length(F.trim(F.col(\"url_raw\"))) > 0))\n",
    "    .where(F.col(\"site_raw\").isNotNull() & (F.length(F.trim(F.col(\"site_raw\"))) > 0))\n",
    "    .dropDuplicates([\"site_raw\", \"url_raw\"])\n",
    ")\n",
    "\n",
    "display(input_df.limit(50))\n",
    "print(\"Rows to scrape:\", input_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2bf9501-0652-4a12-8334-464cd206b4d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n# FINAL CELL (COPY/PASTE REPLACEMENT) — FIXES CANNOT_DETERMINE_TYPE\n# Key fix: we explicitly define a Spark schema (no inference).\n# Input: Spark DF `input_df` with columns: site_raw, url_raw\n# Output: Spark DF `out_spark` + per-site status summary\n# ============================================================\n\nimport re, time, datetime, unicodedata\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\nfrom pyspark.sql.types import (\n    StructType, StructField,\n    StringType, IntegerType, LongType, BooleanType\n)\nfrom pyspark.sql import functions as F\n\n# ---------------- Config ----------------\nMAX_SITE_WORKERS = 10              # parallel across sites\nMAX_URL_WORKERS_PER_SITE = 4       # parallel within a site\nDEBUG_LIMIT = None                 # set e.g. 200 while debugging, None for full run\n\n# ---------------- Helpers ----------------\ndef _now_utc_iso():\n    return datetime.datetime.now(datetime.timezone.utc).isoformat()\n\ndef _strip_accents(s: str) -> str:\n    if s is None:\n        return \"\"\n    s = str(s)\n    return \"\".join(c for c in unicodedata.normalize(\"NFKD\", s) if not unicodedata.combining(c))\n\nimport re\n\ndef parse_price_cop(x) -> int:\n    \"\"\"\n    Parse COP prices robustly.\n    Handles:\n      - $79.990\n      - $79.990,00\n      - 79990.0\n      - 99.000,00\n      - 108,900.00\n    Returns integer pesos (no cents).\n    \"\"\"\n    if x is None:\n        return 0\n\n    s = str(x).strip()\n    if not s:\n        return 0\n\n    # keep digits + separators only\n    s = re.sub(r\"[^\\d,\\.]\", \"\", s)\n    if not s:\n        return 0\n\n    # If we have a decimal part (1-2 digits), drop it.\n    # Examples:\n    # 99.000,00  -> decimal ',' + 2 digits\n    # 108,900.00 -> decimal '.' + 2 digits\n    # 79990.0    -> decimal '.' + 1 digit\n    m = re.search(r\"([,\\.])(\\d{1,2})$\", s)\n    if m:\n        s = s[:m.start()]  # remove decimal part\n\n    # Now remove thousands separators (any remaining '.' or ',')\n    s = re.sub(r\"[^\\d]\", \"\", s)\n\n    try:\n        return int(s) if s else 0\n    except Exception:\n        return 0\n\nSITE_ALIASES = {\n    \"farmatodo\": {\"farmatodo\", \"farmatodo.com.co\"},\n    \"mercadolibre\": {\"mercado libre\", \"mercadolibre\", \"mercado_libre\", \"ml\"},\n    \"sinintermediarios\": {\"sin intermediarios\", \"sinintermediarios\", \"sin-intermediarios\"},\n    \"colsubsidio\": {\"colsubsidio\", \"droguerias colsubsidio\", \"drogueriascolsubsidio\"},\n    \"nutramerican\": {\"nutramerican\", \"nutramerican pharma\", \"nutramericanpharma\"},\n    \"proscience\": {\"proscience\"},\n    \"vitanas\": {\"vitanas\"},\n    \"herbivore\": {\"herbivore\"},\n    \"savvy\": {\"savvy\", \"youaresavvy\", \"you are savvy\"},\n    \"muscletech\": {\"muscletech\"},\n    \"zonafit\": {\"zonafit\", \"zona fit\", \"zona_fit\"},\n}\n\ndef normalize_site(site_raw: str) -> str:\n    if not site_raw:\n        return None\n    s = _strip_accents(site_raw).lower().strip()\n    s = re.sub(r\"\\s+\", \" \", s)\n\n    s = s.replace(\"mercado libre\", \"mercadolibre\")\n    s = s.replace(\"zona fit\", \"zonafit\")\n    s = s.replace(\"sin intermediarios\", \"sinintermediarios\")\n\n    for canon, aliases in SITE_ALIASES.items():\n        if s == canon or s in aliases:\n            return canon\n\n    s2 = re.sub(r\"[^a-z0-9]+\", \"\", s)\n    for canon, aliases in SITE_ALIASES.items():\n        if s2 == canon or s2 in {re.sub(r\"[^a-z0-9]+\", \"\", a) for a in aliases}:\n            return canon\n\n    return s2 or None\n\n# ---------------- Output schema (NO inference) ----------------\nOUT_SCHEMA = StructType([\n    StructField(\"site_raw\", StringType(), True),\n    StructField(\"site\", StringType(), True),\n    StructField(\"url\", StringType(), True),\n\n    StructField(\"status\", StringType(), True),\n    StructField(\"error\", StringType(), True),\n    StructField(\"http_status\", IntegerType(), True),\n    StructField(\"elapsed_ms\", IntegerType(), True),\n    StructField(\"scraped_at\", StringType(), True),\n\n    StructField(\"price\",       LongType(),    True),  # bigint: avoids OverflowError on large COP prices\n    StructField(\"promo_price\", LongType(),    True),\n    StructField(\"membership\",  LongType(),    True),\n\n    StructField(\"has_stock\", BooleanType(), True),\n    StructField(\"stock\", StringType(), True),\n\n    StructField(\"name_scraped\", StringType(), True),\n    StructField(\"image_url\", StringType(), True),\n])\n\ndef _row_with_defaults(**kwargs):\n    \"\"\"\n    Ensures every row has ALL OUT_SCHEMA fields with correct types,\n    preventing Spark from failing schema inference.\n    \"\"\"\n    base = {\n        \"site_raw\": None, \"site\": None, \"url\": None,\n        \"status\": None, \"error\": None, \"http_status\": None, \"elapsed_ms\": None, \"scraped_at\": None,\n        \"price\": 0, \"promo_price\": 0, \"membership\": 0,\n        \"has_stock\": None, \"stock\": None,\n        \"name_scraped\": None, \"image_url\": None,\n    }\n    base.update(kwargs)\n\n    # force-cast numeric fields safely\n    for k in [\"http_status\", \"elapsed_ms\", \"price\", \"promo_price\", \"membership\"]:\n        v = base.get(k)\n        if v is None:\n            base[k] = None\n        else:\n            try:\n                base[k] = int(v)\n            except Exception:\n                base[k] = None if k in [\"http_status\", \"elapsed_ms\"] else 0\n\n    # has_stock must be bool or None\n    hs = base.get(\"has_stock\")\n    if hs is not None and not isinstance(hs, bool):\n        if str(hs).lower() in (\"true\", \"1\", \"yes\", \"y\"):\n            base[\"has_stock\"] = True\n        elif str(hs).lower() in (\"false\", \"0\", \"no\", \"n\"):\n            base[\"has_stock\"] = False\n        else:\n            base[\"has_stock\"] = None\n\n    return base\n\n# ---------------- Input ----------------\nif \"input_df\" not in globals():\n    raise ValueError(\"input_df is not defined. Run the cell that creates input_df (site_raw/url_raw) first.\")\n\ndf_for_scrape = input_df.select(\"site_raw\", \"url_raw\")\nif DEBUG_LIMIT:\n    df_for_scrape = df_for_scrape.limit(DEBUG_LIMIT)\n\nto_scrape_rows = [r.asDict() for r in df_for_scrape.collect()]\n\n# ---------------- Scrape one URL ----------------\ndef scrape_one(site_raw: str, url_raw: str):\n    site_key = normalize_site(site_raw)\n\n    if not site_key or not url_raw:\n        return _row_with_defaults(\n            site_raw=site_raw, site=site_key, url=url_raw,\n            status=\"invalid_input\",\n            error=f\"missing site/url (site_raw='{site_raw}', url='{url_raw}')\",\n            scraped_at=_now_utc_iso(),\n        )\n\n    scraper = SITE_TO_SCRAPER.get(site_key) if \"SITE_TO_SCRAPER\" in globals() else None\n    if not scraper:\n        return _row_with_defaults(\n            site_raw=site_raw, site=site_key, url=url_raw,\n            status=\"unsupported_site\",\n            error=f\"unsupported site key '{site_key}' (raw='{site_raw}')\",\n            scraped_at=_now_utc_iso(),\n        )\n\n    t0 = time.time()\n    try:\n        out = scraper.scrape(url_raw) if hasattr(scraper, \"scrape\") else scraper(url_raw)\n\n        price = parse_price_cop(out.get(\"price\"))\n        promo = parse_price_cop(out.get(\"promo_price\"))\n        member = parse_price_cop(out.get(\"membership\"))\n\n        status = out.get(\"status\") or \"ok\"\n        # if we extracted a price, we treat as ok unless explicitly errored\n        if status not in (\"exception\", \"error\", \"http_error\") and (price > 0 or promo > 0 or member > 0):\n            status = \"ok\"\n\n        return _row_with_defaults(\n            site_raw=site_raw,\n            site=site_key,\n            url=url_raw,\n            status=status,\n            error=out.get(\"error\"),\n            http_status=out.get(\"http_status\"),\n            elapsed_ms=int((time.time() - t0) * 1000),\n            scraped_at=out.get(\"scraped_at\") or _now_utc_iso(),\n            price=price,\n            promo_price=promo,\n            membership=member,\n            has_stock=out.get(\"has_stock\"),\n            stock=out.get(\"stock\"),\n            name_scraped=out.get(\"name_scraped\"),\n            image_url=out.get(\"image_url\"),\n        )\n\n    except Exception as e:\n        return _row_with_defaults(\n            site_raw=site_raw, site=site_key, url=url_raw,\n            status=\"exception\",\n            error=repr(e),\n            elapsed_ms=int((time.time() - t0) * 1000),\n            scraped_at=_now_utc_iso(),\n        )\n\n# ---------------- Group by site ----------------\nby_site = {}\nfor r in to_scrape_rows:\n    sr = r.get(\"site_raw\")\n    u = r.get(\"url_raw\")\n    sk = normalize_site(sr)\n    by_site.setdefault(sk or \"unknown\", []).append((sr, u))\n\nsites = sorted(by_site.keys())\nprint(\"Sites:\", sites)\n\n# ---------------- Parallel execution ----------------\nMAX_SITE_WORKERS_EFFECTIVE = min(MAX_SITE_WORKERS, max(1, len(by_site)))\n\ndef scrape_site(site_key, pairs):\n    out = []\n    with ThreadPoolExecutor(max_workers=MAX_URL_WORKERS_PER_SITE) as url_pool:\n        futs = [url_pool.submit(scrape_one, sr, u) for (sr, u) in pairs]\n        for f in as_completed(futs):\n            out.append(f.result())\n    return out\n\nresults = []\nwith ThreadPoolExecutor(max_workers=MAX_SITE_WORKERS_EFFECTIVE) as site_pool:\n    futs = {site_pool.submit(scrape_site, k, v): k for k, v in by_site.items()}\n    for f in as_completed(futs):\n        results.extend(f.result())\n\n# ---------------- Output (schema-fixed) ----------------\nout_spark = spark.createDataFrame(results, schema=OUT_SCHEMA)\ndisplay(out_spark)\n\n# ---------------- Summary (ok per commerce) ----------------\nsite_summary = (\n    out_spark\n    .groupBy(\"site\")\n    .agg(\n        F.count(\"*\").alias(\"rows\"),\n        F.sum(F.when(F.col(\"status\") == \"ok\", 1).otherwise(0)).alias(\"ok_rows\"),\n        F.sum(F.when(F.col(\"status\") == \"unsupported_site\", 1).otherwise(0)).alias(\"unsupported_rows\"),\n        F.sum(F.when(F.col(\"status\") == \"exception\", 1).otherwise(0)).alias(\"exception_rows\"),\n        F.sum(F.when(F.col(\"status\") == \"invalid_input\", 1).otherwise(0)).alias(\"invalid_rows\"),\n        F.sum(\n            F.when(\n                (F.col(\"status\") == \"ok\") &\n                (F.col(\"price\") <= 0) & (F.col(\"promo_price\") <= 0) & (F.col(\"membership\") <= 0),\n                1\n            ).otherwise(0)\n        ).alias(\"ok_but_no_price_rows\"),\n    )\n    .withColumn(\"ok_rate\", F.round(F.col(\"ok_rows\") / F.col(\"rows\"), 3))\n    .orderBy(F.desc(\"rows\"))\n)\ndisplay(site_summary)\n\n# ---------------- Error sample ----------------\nerrors_df = (\n    out_spark\n    .filter(F.col(\"status\").isin(\"exception\", \"unsupported_site\", \"invalid_input\"))\n    .select(\"site\", \"site_raw\", \"status\", \"error\", \"url\")\n)\ndisplay(errors_df.limit(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bc48931-76f6-4cf1-b728-3cb84961cf03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3.5) MercadoLibre \"pseudo-scrape\" from MAIN_TABLE (main_file)\n",
    "# Paste BETWEEN the scrape driver cell (creates out_spark) and Cell 10 (schema normalization).\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "\n",
    "if \"out_spark\" not in globals():\n",
    "    raise ValueError(\"out_spark not found. Run the scrape driver cell first (the cell that produces out_spark).\")\n",
    "\n",
    "# --- Load MAIN_TABLE (catalog / manual ML prices source) ---\n",
    "main_df = spark.table(MAIN_TABLE)\n",
    "cols_lc = {c.lower(): c for c in main_df.columns}\n",
    "\n",
    "# Required cols\n",
    "url_col  = cols_lc.get(\"url\")\n",
    "site_col = cols_lc.get(\"comercio\") or cols_lc.get(\"site\")\n",
    "if not url_col or not site_col:\n",
    "    raise ValueError(f\"MAIN_TABLE must contain url + comercio/site. Found: {main_df.columns}\")\n",
    "\n",
    "# MercadoLibre price columns (as you confirmed)\n",
    "precio_full_col      = cols_lc.get(\"precio_full\")\n",
    "precio_membresia_col = cols_lc.get(\"precio_membresia\")\n",
    "precio_dcto_col      = cols_lc.get(\"precio_dcto\")\n",
    "\n",
    "missing = [x for x in [\"precio_full\",\"precio_membresia\",\"precio_dcto\"] if x not in cols_lc]\n",
    "if missing:\n",
    "    raise ValueError(f\"MAIN_TABLE is missing required MercadoLibre price columns: {missing}. Found: {main_df.columns}\")\n",
    "\n",
    "# Optional enrich columns (only used if present)\n",
    "name_col  = cols_lc.get(\"producto\") or cols_lc.get(\"name\") or cols_lc.get(\"nombre\")\n",
    "image_col = cols_lc.get(\"image_url\") or cols_lc.get(\"image\") or cols_lc.get(\"imagen\")\n",
    "\n",
    "# Reuse notebook helpers\n",
    "normalize_site_udf = F.udf(normalize_site, StringType())\n",
    "parse_price_udf    = F.udf(parse_price_cop, IntegerType())\n",
    "\n",
    "# Parse prices\n",
    "parsed = (\n",
    "    main_df\n",
    "      .withColumn(\"site_norm\", normalize_site_udf(F.col(site_col).cast(\"string\")))\n",
    "      .filter(F.col(\"site_norm\") == F.lit(\"mercadolibre\"))\n",
    "      .withColumn(\"p_full\", parse_price_udf(F.col(precio_full_col)))\n",
    "      .withColumn(\"p_memb\", parse_price_udf(F.col(precio_membresia_col)))\n",
    "      .withColumn(\"p_dcto\", parse_price_udf(F.col(precio_dcto_col)))\n",
    ")\n",
    "\n",
    "# DROP if regular price is missing or malformed (<=0 or null)\n",
    "parsed_good = parsed.filter(F.col(\"p_full\").isNotNull() & (F.col(\"p_full\") > 0))\n",
    "\n",
    "# Build synthetic rows matching OUT_SCHEMA (from Cell 9)\n",
    "ml_rows = (\n",
    "    parsed_good\n",
    "      .select(\n",
    "          F.col(site_col).cast(\"string\").alias(\"site_raw\"),\n",
    "          F.lit(\"mercadolibre\").alias(\"site\"),\n",
    "          F.col(url_col).cast(\"string\").alias(\"url\"),\n",
    "\n",
    "          # Map main_file columns to scraper output fields\n",
    "          F.col(\"p_full\").cast(\"int\").alias(\"price\"),          # -> downstream price_full_cop\n",
    "          F.coalesce(F.col(\"p_dcto\"), F.lit(0)).cast(\"int\").alias(\"promo_price\"),   # -> downstream price_discount_cop\n",
    "          F.coalesce(F.col(\"p_memb\"), F.lit(0)).cast(\"int\").alias(\"membership\"),    # -> downstream price_membership_cop\n",
    "\n",
    "          # Synthetic scrape metadata\n",
    "          F.lit(\"ok\").alias(\"status\"),\n",
    "          F.lit(None).cast(\"string\").alias(\"error\"),\n",
    "          F.lit(200).cast(\"int\").alias(\"http_status\"),\n",
    "          F.lit(0).cast(\"int\").alias(\"elapsed_ms\"),\n",
    "          F.current_timestamp().cast(\"string\").alias(\"scraped_at\"),\n",
    "\n",
    "          # Optional enrichments\n",
    "          (F.col(name_col).cast(\"string\") if name_col else F.lit(None).cast(\"string\")).alias(\"name_scraped\"),\n",
    "          (F.col(image_col).cast(\"string\") if image_col else F.lit(None).cast(\"string\")).alias(\"image_url\"),\n",
    "\n",
    "          # Unknown for ML manual feed\n",
    "          F.lit(None).cast(\"boolean\").alias(\"has_stock\"),\n",
    "          F.lit(None).cast(\"string\").alias(\"stock\"),\n",
    "      )\n",
    "      .dropDuplicates([\"url\"])\n",
    ")\n",
    "\n",
    "# Enforce exact OUT_SCHEMA column order\n",
    "expected_cols = [f.name for f in OUT_SCHEMA.fields]\n",
    "ml_rows = ml_rows.select(*expected_cols)\n",
    "\n",
    "# Replace any MercadoLibre rows produced by the scraper driver (unsupported_site/no_price/etc.)\n",
    "out_spark = (\n",
    "    out_spark\n",
    "      .filter(F.col(\"site\") != F.lit(\"mercadolibre\"))\n",
    "      .unionByName(ml_rows, allowMissingColumns=True)\n",
    ")\n",
    "\n",
    "print(\"✅ MercadoLibre rows injected from main_file into out_spark (dropping malformed/missing regular prices).\")\n",
    "display(out_spark.filter(F.col(\"site\") == \"mercadolibre\").orderBy(F.col(\"url\")).limit(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bb8c204-1e76-4ac8-a219-11c59da7bef9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4) TO SPARK + NORMALIZE SCHEMA  (NO PANDAS)\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Expecting the scrape driver cell created `out_spark`\n",
    "if \"out_spark\" not in globals():\n",
    "    raise ValueError(\"out_spark is not defined. Run the scrape driver cell first (it produces out_spark).\")\n",
    "\n",
    "scrape_spark = out_spark\n",
    "\n",
    "# Backward compatible columns (downstream expects *_full/_discount/_membership)\n",
    "# Our driver outputs: price, promo_price, membership (all ints COP)\n",
    "if \"price_full\" not in scrape_spark.columns:\n",
    "    scrape_spark = scrape_spark.withColumn(\"price_full\", F.col(\"price\").cast(\"long\"))\n",
    "if \"price_discount\" not in scrape_spark.columns:\n",
    "    scrape_spark = scrape_spark.withColumn(\"price_discount\", F.col(\"promo_price\").cast(\"long\"))\n",
    "if \"price_membership\" not in scrape_spark.columns:\n",
    "    scrape_spark = scrape_spark.withColumn(\"price_membership\", F.col(\"membership\").cast(\"long\"))\n",
    "\n",
    "# Add scrape_date + COP long columns\n",
    "scrape_spark = (\n",
    "    scrape_spark\n",
    "      .withColumn(\"scrape_date\", F.to_date(F.current_timestamp()))\n",
    "      .withColumn(\"price_full_cop\", F.col(\"price_full\").cast(\"long\"))\n",
    "      .withColumn(\"price_discount_cop\", F.col(\"price_discount\").cast(\"long\"))\n",
    "      .withColumn(\"price_membership_cop\", F.col(\"price_membership\").cast(\"long\"))\n",
    ")\n",
    "\n",
    "# Best price (min positive among discount/membership/full)\n",
    "scrape_spark = scrape_spark.withColumn(\n",
    "    \"price_cop\",\n",
    "    F.least(\n",
    "        F.when(F.col(\"price_discount_cop\") > 0, F.col(\"price_discount_cop\")),\n",
    "        F.when(F.col(\"price_full_cop\") > 0, F.col(\"price_full_cop\")),\n",
    "    )\n",
    ")\n",
    "\n",
    "# If all are null (no positive prices), set to 0\n",
    "scrape_spark = scrape_spark.withColumn(\"price_cop\", F.coalesce(F.col(\"price_cop\"), F.lit(0).cast(\"long\")))\n",
    "\n",
    "# De-dup within this run to keep one row per (scrape_date, url)\n",
    "# Use scraped_at if present; otherwise fallback to current_timestamp ordering\n",
    "order_col = F.col(\"scraped_at\").desc() if \"scraped_at\" in scrape_spark.columns else F.current_timestamp().desc()\n",
    "w = Window.partitionBy(\"scrape_date\", \"url\").orderBy(order_col)\n",
    "\n",
    "scrape_spark = (\n",
    "    scrape_spark\n",
    "      .withColumn(\"_rn\", F.row_number().over(w))\n",
    "      .filter(F.col(\"_rn\") == 1)\n",
    "      .drop(\"_rn\")\n",
    ")\n",
    "\n",
    "display(\n",
    "    scrape_spark.select(\n",
    "        \"site\",\"url\",\n",
    "        \"price_full_cop\",\"price_discount_cop\",\"price_membership_cop\",\"price_cop\",\n",
    "        \"status\",\"error\"\n",
    "    ).limit(200)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df0e41a5-8291-4b12-8abc-8dbcc4fedeee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "# 5) WRITE SCRAPE SNAPSHOT (append)\n\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import StringType\n\ndef table_exists(full_name: str) -> bool:\n    try:\n        spark.table(full_name)\n        return True\n    except Exception:\n        return False\n\ndef align_schema_to_table(df, table_name: str):\n    \"\"\"\n    Cast df columns to match the existing Delta table schema.\n    Prevents DELTA_FAILED_TO_MERGE_FIELDS when a column type changed\n    (e.g. price went from StringType -> LongType).\n    Only casts columns that already exist in both df and table.\n    \"\"\"\n    existing = spark.table(table_name).schema\n    for field in existing:\n        if field.name in df.columns:\n            df = df.withColumn(field.name, F.col(field.name).cast(field.dataType))\n    return df\n\nif not table_exists(SCRAPE_TABLE):\n    (scrape_spark\n      .write.format('delta')\n      .mode('overwrite')\n      .partitionBy('scrape_date')\n      .saveAsTable(SCRAPE_TABLE))\n    print(f'Created {SCRAPE_TABLE}')\nelse:\n    aligned = align_schema_to_table(scrape_spark, SCRAPE_TABLE)\n    (aligned\n      .write.format('delta')\n      .mode('append')\n      .option('mergeSchema', 'true')\n      .saveAsTable(SCRAPE_TABLE))\n    print(f'Appended to {SCRAPE_TABLE}')\n\n# Quick peek\ndisplay(spark.table(SCRAPE_TABLE).orderBy(F.col('scrape_date').desc(), F.col('site')).limit(200))"
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_SinIntermediarios_Daily_Scrape_Bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}